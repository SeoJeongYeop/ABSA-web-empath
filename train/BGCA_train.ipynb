{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-02T15:20:39.094125Z","iopub.execute_input":"2023-11-02T15:20:39.094743Z","iopub.status.idle":"2023-11-02T15:20:39.125738Z","shell.execute_reply.started":"2023-11-02T15:20:39.094714Z","shell.execute_reply":"2023-11-02T15:20:39.124642Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/bgca-ko/aste/cross_domain/youtube/test.txt\n/kaggle/input/bgca-ko/aste/cross_domain/youtube/train.txt\n/kaggle/input/bgca-ko/aste/cross_domain/youtube/dev.txt\n/kaggle/input/bgca-ko/aste/cross_domain/blog/test.txt\n/kaggle/input/bgca-ko/aste/cross_domain/blog/train.txt\n/kaggle/input/bgca-ko/aste/cross_domain/blog/dev.txt\n/kaggle/input/bgca-ko/aste/cross_domain/news/test.txt\n/kaggle/input/bgca-ko/aste/cross_domain/news/train.txt\n/kaggle/input/bgca-ko/aste/cross_domain/news/dev.txt\n/kaggle/input/bgca-ko/aste/cross_domain/merge/test.txt\n/kaggle/input/bgca-ko/aste/cross_domain/merge/train.txt\n/kaggle/input/bgca-ko/aste/cross_domain/merge/dev.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Requirements","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.18.0 -q","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:20:39.127472Z","iopub.execute_input":"2023-11-02T15:20:39.127776Z","iopub.status.idle":"2023-11-02T15:21:01.920244Z","shell.execute_reply.started":"2023-11-02T15:20:39.127749Z","shell.execute_reply":"2023-11-02T15:21:01.918809Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install sentencepiece==0.1.96 -q","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:01.921834Z","iopub.execute_input":"2023-11-02T15:21:01.922137Z","iopub.status.idle":"2023-11-02T15:21:15.212010Z","shell.execute_reply.started":"2023-11-02T15:21:01.922108Z","shell.execute_reply":"2023-11-02T15:21:15.210726Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch_lightning==0.8.1 -q","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:15.215136Z","iopub.execute_input":"2023-11-02T15:21:15.215485Z","iopub.status.idle":"2023-11-02T15:21:28.954975Z","shell.execute_reply.started":"2023-11-02T15:21:15.215453Z","shell.execute_reply":"2023-11-02T15:21:28.953855Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install editdistance==0.6.0 -q","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:28.956482Z","iopub.execute_input":"2023-11-02T15:21:28.956830Z","iopub.status.idle":"2023-11-02T15:21:47.617803Z","shell.execute_reply.started":"2023-11-02T15:21:28.956801Z","shell.execute_reply":"2023-11-02T15:21:47.616399Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# !pip install numpy==1.22.3 -q\n# !pip install scikit-learn==0.24.2 -q\n# !pip install tqdm==4.64.0 -q","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:47.619609Z","iopub.execute_input":"2023-11-02T15:21:47.619984Z","iopub.status.idle":"2023-11-02T15:21:47.625047Z","shell.execute_reply.started":"2023-11-02T15:21:47.619950Z","shell.execute_reply":"2023-11-02T15:21:47.624043Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 모델","metadata":{}},{"cell_type":"code","source":"# constants.py\nT5_ORI_LEN = 64100 #T5_ORI_LEN = 32100\n\nTAG_TO_WORD = {'POS': 'positive', 'NEG': 'negative', 'NEU': 'neutral'}\nTAG_WORD_LIST = ['positive', 'negative', 'neutral']\n\n# annotation-bracket\nTAG_TO_BRACKET = {\"POS\": (\"((\", \"))\"), \"NEG\": (\"[[\", \"]]\"), \"NEU\": (\"{{\", \"}}\")}\nBRACKET_TO_TAG = {\"{{\": \"neutral\", \"[[\": \"negative\", \"((\": \"positive\"}\n\n# annotation-special\nNONE_TOKEN = \"[none]\"\nAND_TOKEN = \"[and]\"\nASPECT_TOKEN = \"<aspect>\"\nOPINION_TOKEN = \"<opinion>\"\nEMPTY_TOKEN = \"<empty>\"\nSEP_TOKEN = \"<sep>\"\nTAG_TO_SPECIAL = {\"POS\": (\"<pos>\", \"</pos>\"), \"NEG\": (\"<neg>\", \"</neg>\"), \"NEU\": (\"<neu>\", \"</neu>\")}\nSPECIAL_TO_TAG = {\"<pos>\": \"positive\", \"<neg>\": \"negative\", \"<neu>\": \"neutral\"}\n\n# TARGET_TEST_COUNT_DICT = {\n#     \"laptop\": 800,\n#     \"rest\": 2158,\n#     \"device\": 1279,\n#     \"service\": 747,\n# }\n\nTARGET_TEST_COUNT_DICT = {\n    \"blog\": 159,\n    \"merge\": 103\n}\n\n# please follw L, R, D, S order for evaluation purpose, since we will combine test files into one\nUABSA_TRANSFER_PAIRS = {\n    \"laptop\": [\"rest\", \"service\"],\n    \"rest\": [\"laptop\", \"device\", \"service\"],\n    \"device\": [\"rest\", \"service\"],\n    \"service\": [\"laptop\", \"rest\", \"device\"],\n}\nASTE_TRANSFER_PAIRS = {\n    \"blog\": [\"merge\"],\n    \"merge\": [\"blog\"]\n}\n# ASTE_TRANSFER_PAIRS = {\n#     \"rest14\": [\"laptop14\"],\n#     \"rest15\": [\"laptop14\"],\n#     \"rest16\": [\"laptop14\"],\n#     \"laptop14\": [\"rest14\", \"rest15\", \"rest16\"],\n# }\nAOPE_TRANSFER_PAIRS = ASTE_TRANSFER_PAIRS\n\nSTOP_WORDS = []#['about', 'itself', 'so', 'further', 'against', \"don't\", 'shouldn', 'to', 'didn', 'hers', 'over', 'haven', \"it's\", 'of', 'have', 'm', 'but', \"you've\", 'which', 'd', 'most', 'nor', \"haven't\", \"wasn't\", 'yourself', 'with', 'am', 'do', 'than', \"that'll\", \"isn't\", 'or', \"shan't\", 'then', 'while', 'did', 'off', 'under', \"mustn't\", \"won't\", 'again', 'you', 'its', 'these', 'some', 'he', 'after', 'doesn', 'into', 't', 'more', 'whom', 'his', 'from', 'a', 'at', 'during', 'when', \"she's\", \"aren't\", 'was', 'same', 'myself', 'my', 'has', 'aren', 'by', 'before', \"needn't\", 'yourselves', 'such', 'she', 'is', 'needn', 'here', 'too', 'ourselves', \"didn't\", 'both', 'i', 'theirs', 'weren', 'be', 'their', 'were', 'because', 'should', \"should've\", \"couldn't\", 'will', 'isn', 'all', 'and', 'through', 'won', \"weren't\", 'y', 'they', 'for', 'until', 'him', 's', 'now', 'those', 'up', 'had', 'that', 'ma', 'couldn', 'been', 'why', 'below', 'own', 'doing', \"you'll\", 'very', 'above', \"shouldn't\", 'where', 've', 'if', 'are', 'how', 'wasn', 'it', 'what', 'as', 'hadn', 'hasn', \"you'd\", \"wouldn't\", 'don', 'few', 'other', 're', 'ain', \"hadn't\", \"doesn't\", 'himself', 'shan', 'the', 'not', 'mustn', 'does', \"hasn't\", 'll', 'your', 'yours', 'herself', 'in', 'wouldn', 'themselves', 'who', 'there', 'ours', 'out', 'mightn', 'me', 'them', 'once', \"mightn't\", 'we', 'her', 'this', 'being', 'any', 'can', 'o', 'no', 'having', \"you're\", 'our', 'on', 'between', 'down', 'only', 'just', 'each', 'an']","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:47.626480Z","iopub.execute_input":"2023-11-02T15:21:47.626814Z","iopub.status.idle":"2023-11-02T15:21:47.646556Z","shell.execute_reply.started":"2023-11-02T15:21:47.626785Z","shell.execute_reply":"2023-11-02T15:21:47.645477Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:47.647834Z","iopub.execute_input":"2023-11-02T15:21:47.648126Z","iopub.status.idle":"2023-11-02T15:21:51.137920Z","shell.execute_reply.started":"2023-11-02T15:21:47.648093Z","shell.execute_reply":"2023-11-02T15:21:51.136792Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nlogger = logging.getLogger(\"BGCA\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:51.139473Z","iopub.execute_input":"2023-11-02T15:21:51.140013Z","iopub.status.idle":"2023-11-02T15:21:51.145188Z","shell.execute_reply.started":"2023-11-02T15:21:51.139973Z","shell.execute_reply":"2023-11-02T15:21:51.144286Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#data_utils.py\n# This file contains all data loading and transformation functions\n\nimport random\nimport re\n\nfrom torch.utils.data import Dataset\n\n\nsenttag2word = {'POS': 'positive', 'NEG': 'negative', 'NEU': 'neutral'}\n\n\ndef filter_invalid(inputs, outputs):\n    new_inputs, new_outputs = [], []\n    rm_num = 0\n    for idx in range(len(inputs)):\n        valid = True\n        aps = re.findall(\"(<\\w+>)(.*?)(?=<\\w+>|$)\", outputs[idx])\n        for ap in aps:\n            for ele in ap:\n                # some element is missing\n                if len(ele) == 0:\n                    valid = False\n                    break\n            if valid is False:\n                break\n        if valid:\n            new_inputs.append(inputs[idx])\n            new_outputs.append(outputs[idx])\n        else:\n            rm_num += 1\n            logger.info(f\"Output is invalid: {outputs[idx]}\")\n    logger.info(f\"Filterd out {rm_num} invalid samples\")\n    return new_inputs, new_outputs\n\n\ndef filter_none(inputs, outputs, ratio=0):\n    if ratio > 0:\n        new_inputs = []\n        new_outputs = []\n        rm_num = 0\n        none_num = 0\n        import random\n        for idx in range(len(inputs)):\n            if outputs[idx].strip() == NONE_TOKEN:\n                none_num += 1\n                if random.random() < ratio:\n                    rm_num += 1\n                    continue\n            new_inputs.append(inputs[idx])\n            new_outputs.append(outputs[idx])\n        logger.info(f\"Filtered out {rm_num} [none] samples out of {none_num} [none]\")\n        return new_inputs, new_outputs\n    else:\n        return inputs, outputs\n\n\ndef normalize_augment(args, inputs, outputs):\n    new_inputs = []\n    new_outputs = []\n    rm_num = 0\n    for idx in range(len(inputs)):\n        # rm added random word\n        if args.data_gene_none_word_num > 0 and NONE_TOKEN in outputs[idx]:\n            if args.task in [\"uabsa\", \"ate\"]:\n                # just keep none token\n                outputs[idx] = NONE_TOKEN\n        new_inputs.append(inputs[idx])\n        new_outputs.append(outputs[idx])\n    return new_inputs, new_outputs\n\n\ndef read_line_examples_from_file(data_path):\n    \"\"\"\n    Read data from file, each line is: sent####labels\n    Return List[List[word]], List[Tuple]\n    \"\"\"\n    sents, labels = [], []\n    with open(data_path, 'r', encoding='UTF-8') as fp:\n        words, labels = [], []\n        for line in fp:\n            line = line.strip()\n            if line != '':\n                words, tuples = line.split('####')\n                sents.append(words.split())\n                labels.append(eval(tuples))\n    logger.info(f\"{data_path.split('/')[-1]}\\tTotal examples = {len(sents)} \")\n    return sents, labels\n\n\nclass ABSADataset(Dataset):\n    def __init__(self, args, tokenizer, inputs=None, targets=None, dataset_list=None, name=None):\n        self.args = args\n        self.tokenizer = tokenizer\n        self.inputs, self.targets = inputs or [], targets or []\n        self.inputs_tensor_list, self.targets_tensor_list = [], []\n        self.name = name or []\n\n        # If a dataset_list is provided, merge the datasets\n        if dataset_list:\n            for d in dataset_list:\n                self.inputs += d.inputs\n                self.targets += d.targets\n                self.inputs_tensor_list += d.inputs_tensor_list\n                self.targets_tensor_list += d.targets_tensor_list\n\n                if isinstance(d.name, list):\n                    self.name.extend(d.name)\n                else:\n                    self.name.append(d.name)\n\n            # Write merged data to a file\n            processed_path = f\"{d.args.data_dir}/{'+'.join(self.name)}_merged_processed.txt\"\n            with open(processed_path, \"w\", encoding='utf-8') as f:\n                for i in range(len(self.inputs)):\n                    f.write(f\"{self.inputs[i]} ===> {self.targets[i]}\\n\")\n            logger.info(f\"{self.name} is merged.\")\n            logger.info(f\"{processed_path} is written.\")\n        else:\n            # If no dataset_list is provided, behave like ABSADataset\n            self.max_len = args.max_seq_length\n            self.inputs_tensor_list, self.targets_tensor_list = self.encode(self.inputs, self.targets)\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, index):\n        source_ids = self.inputs_tensor_list[index][\"input_ids\"].squeeze()\n        target_ids = self.targets_tensor_list[index][\"input_ids\"].squeeze()\n\n        src_mask = self.inputs_tensor_list[index][\"attention_mask\"].squeeze()\n        target_mask = self.targets_tensor_list[index][\"attention_mask\"].squeeze()\n\n        return {\"source_ids\": source_ids, \"source_mask\": src_mask,\n                \"target_ids\": target_ids, \"target_mask\": target_mask}\n\n    def encode(self, inputs=[], targets=[]):\n        inputs_tensor_list, targets_tensor_list = [], []\n\n        for i in range(len(inputs)):\n            input_i = ' '.join(inputs[i]) if isinstance(inputs[i], list) else inputs[i]\n            target_i = ' '.join(targets[i]) if isinstance(targets[i], list) else targets[i]\n\n            # Tokenize input and target data\n            tokenized_input = self.tokenizer.batch_encode_plus(\n                [input_i], max_length=self.max_len, padding='max_length', truncation=True,\n                return_tensors=\"pt\",\n            )\n            tokenized_target = self.tokenizer.batch_encode_plus(\n                [target_i], max_length=self.max_len, padding='max_length', truncation=True,\n                return_tensors=\"pt\"\n            )\n\n            inputs_tensor_list.append(tokenized_input)\n            targets_tensor_list.append(tokenized_target)\n\n        return inputs_tensor_list, targets_tensor_list\n\n\ndef get_inputs(args, data_type_file=\"train\"):\n    \"\"\"\n        train_inputs: [\"hi\", \"I love apples.\"],\n    \"\"\"\n    data_path = f\"{args.data_dir}/{data_type_file}.txt\"\n    inputs, _ = read_line_examples_from_file(data_path)\n    inputs = [\" \".join(i) for i in inputs]\n    return inputs\n\n\ndef prepare_uabsa_gene(args, data_type_file=\"train\"):\n    \"\"\"\n        input:  I love apple.\n        target: <pos> apple <opinion> love\n    \"\"\"\n    inputs, targets = prepare_uabsa_universal(args, data_type_file=data_type_file)\n    labels, texts = targets, inputs\n    for idx in range(len(labels)):\n        # add random tokens for None\n        if NONE_TOKEN in labels[idx]:\n            words = texts[idx].split()\n            sample_num = min(len(words), args.data_gene_none_word_num)\n            random_words = \" \".join(random.sample(words, sample_num))\n            labels[idx] += \" \" + random_words\n    return labels, texts\n\n\ndef prepare_uabsa_extraction(args, data_type_file=\"train\"):\n    data_path = f\"{args.data_dir}/{data_type_file}.txt\"\n    sents, labels = read_line_examples_from_file(data_path)\n    inputs = [\" \".join(s) for s in sents]\n\n    targets = []\n    for i, label in enumerate(labels):\n        if label == []:\n            targets.append('None')\n        else:\n            all_tri = []\n            for tri in label:\n                # single aspect\n                if len(tri[0]) == 1:\n                    a = sents[i][tri[0][0]]\n                else:\n                    start_idx, end_idx = tri[0][0], tri[0][-1]\n                    a = ' '.join(sents[i][start_idx:end_idx+1])\n                c = TAG_TO_WORD[tri[1]]\n                all_tri.append((a, c))\n            label_strs = ['( '+' , '.join(l)+' )' for l in all_tri]\n            targets.append(' ; '.join(label_strs))\n    return inputs, targets\n\n\ndef prepare_uabsa_universal(args, data_type_file=\"train\"):\n    data_path = f\"{args.data_dir}/{data_type_file}.txt\"\n    sents, labels = read_line_examples_from_file(data_path)\n    inputs = [\" \".join(s) for s in sents]\n\n    targets = []\n    for i, label in enumerate(labels):\n        if label == []:\n            targets.append(f\"{NONE_TOKEN}\")\n        else:\n            target_str = \"\"\n            for tri in label:\n                tag = tri[1]\n                if len(tri[0]) == 1:\n                    aspect = sents[i][tri[0][0]]\n                else:\n                    start_idx, end_idx = tri[0][0], tri[0][-1]\n                    aspect = ' '.join(sents[i][start_idx: end_idx+1])\n                tag_token = TAG_TO_SPECIAL[tag][0]\n                target_str += f\" {tag_token} {aspect}\"\n            targets.append(target_str)\n\n    return inputs, targets\n\n\ndef prepare_ate_extraction(args, data_type_file=\"train\"):\n    data_path = f\"{args.data_dir}/{data_type_file}.txt\"\n    sents, labels = read_line_examples_from_file(data_path)\n    inputs = [\" \".join(s) for s in sents]\n\n    targets = []\n    for i, label in enumerate(labels):\n        if label == []:\n            targets.append('None')\n        else:\n            all_tri = []\n            for tri in label:\n                # single aspect\n                if len(tri[0]) == 1:\n                    a = sents[i][tri[0][0]]\n                else:\n                    start_idx, end_idx = tri[0][0], tri[0][-1]\n                    a = ' '.join(sents[i][start_idx:end_idx+1])\n                all_tri.append(a)\n            label_strs = [f\"( {l} )\" for l in all_tri]\n            targets.append(' ; '.join(label_strs))\n    return inputs, targets\n\n\ndef prepare_ate_universal(args, data_type_file=\"train\"):\n    data_path = f\"{args.data_dir}/{data_type_file}.txt\"\n    sents, labels = read_line_examples_from_file(data_path)\n    inputs = [\" \".join(s) for s in sents]\n\n    targets = []\n    for i, label in enumerate(labels):\n        if label == []:\n            targets.append(f\"{NONE_TOKEN}\")\n        else:\n            target_str = \"\"\n            for tri in label:\n                tag = tri[1]\n                if len(tri[0]) == 1:\n                    aspect = sents[i][tri[0][0]]\n                else:\n                    start_idx, end_idx = tri[0][0], tri[0][-1]\n                    aspect = ' '.join(sents[i][start_idx: end_idx+1])\n                tag_token = ASPECT_TOKEN\n                target_str += f\" {tag_token} {aspect}\"\n            targets.append(target_str)\n\n    return inputs, targets\n\n\ndef prepare_ate_gene(args, data_type_file):\n    # just change order\n    texts, labels = prepare_ate_universal(args, data_type_file=data_type_file)\n    # add a random word to NONE\n    for idx in range(len(labels)):\n        if NONE_TOKEN in labels[idx]:\n            words = texts[idx].split()\n            sample_num = min(len(words), args.data_gene_none_word_num)\n            random_words = \" \".join(random.sample(words, sample_num))\n            labels[idx] += \" \" + random_words\n    return labels, texts\n\n\ndef prepare_aste_extraction(args, data_type_file=\"train\"):\n    data_path = f\"{args.data_dir}/{data_type_file}.txt\"\n    sents, labels = read_line_examples_from_file(data_path)\n    inputs = [\" \".join(s) for s in sents]\n\n    targets = []\n    for i, label in enumerate(labels):\n        all_tri = []\n        for tri in label:\n            if len(tri[0]) == 1:\n                a = sents[i][tri[0][0]]\n            else:\n                start_idx, end_idx = tri[0][0], tri[0][-1]\n                a = ' '.join(sents[i][start_idx:end_idx+1])\n            if len(tri[1]) == 1:\n                b = sents[i][tri[1][0]]\n            else:\n                start_idx, end_idx = tri[1][0], tri[1][-1]\n                b = ' '.join(sents[i][start_idx:end_idx+1])\n            c = senttag2word[tri[2]]\n            all_tri.append((a, b, c))\n        label_strs = ['( '+' , '.join(l)+' )' for l in all_tri]\n        targets.append(' ; '.join(label_strs))\n    return inputs, targets\n\n\ndef prepare_aste_universal(args, data_type_file=\"train\"):\n    data_path = f\"{args.data_dir}/{data_type_file}.txt\"\n    sents, labels = read_line_examples_from_file(data_path)\n    inputs = [ detokenize(\" \".join(s)) for s in sents ]\n\n    targets = []\n    for i, label in enumerate(labels):\n        all_tri = []\n        target_str = \"\"\n        for tri in label:\n            if len(tri[0]) == 1:\n                aspect = sents[i][tri[0][0]]\n            else:\n                start_idx, end_idx = tri[0][0], tri[0][-1]\n                aspect = ' '.join(sents[i][start_idx:end_idx+1])\n            if len(tri[1]) == 1:\n                opinion = sents[i][tri[1][0]]\n            else:\n                start_idx, end_idx = tri[1][0], tri[1][-1]\n                opinion = ' '.join(sents[i][start_idx:end_idx+1])\n            aspect = detokenize(aspect)\n            opinion = detokenize(opinion)\n\n            senti_token = TAG_TO_SPECIAL[tri[2]][0]\n            target_str += f\" {senti_token} {aspect} {OPINION_TOKEN} {opinion}\"\n        targets.append(target_str.strip())\n    return inputs, targets\n\ndef detokenize(sentence: str) -> str:\n    '''\n    토큰화된 인풋을 원래 문장으로 복원\n    '''\n    ret = sentence.replace(\" \", \"\")\n    ret = ret.replace(\"▁\", \" \")\n\n    return ret\n\ndef prepare_aste_gene(args, data_type_file):\n    # just change order, since there is no none\n    targets, inputs = prepare_aste_universal(args, data_type_file=data_type_file)\n    return inputs, targets\n\n\ndef prepare_aope_extraction(args, data_type_file=\"train\"):\n    data_path = f\"{args.data_dir}/{data_type_file}.txt\"\n    sents, labels = read_line_examples_from_file(data_path)\n    inputs = [\" \".join(s) for s in sents]\n\n    targets = []\n    for i, label in enumerate(labels):\n        all_tri = []\n        for tri in label:\n            if len(tri[0]) == 1:\n                a = sents[i][tri[0][0]]\n            else:\n                start_idx, end_idx = tri[0][0], tri[0][-1]\n                a = ' '.join(sents[i][start_idx:end_idx+1])\n            if len(tri[1]) == 1:\n                b = sents[i][tri[1][0]]\n            else:\n                start_idx, end_idx = tri[1][0], tri[1][-1]\n                b = ' '.join(sents[i][start_idx:end_idx+1])\n            all_tri.append((a, b))\n        label_strs = ['( '+' , '.join(l)+' )' for l in all_tri]\n        targets.append(' ; '.join(label_strs))\n\n    return inputs, targets\n\n\ndef prepare_aope_universal(args, data_type_file=\"train\"):\n    data_path = f\"{args.data_dir}/{data_type_file}.txt\"\n    sents, labels = read_line_examples_from_file(data_path)\n    inputs = [\" \".join(s) for s in sents]\n\n    targets = []\n    for i, label in enumerate(labels):\n        all_tri = []\n        target_str = \"\"\n        for tri in label:\n            if len(tri[0]) == 1:\n                a = sents[i][tri[0][0]]\n            else:\n                start_idx, end_idx = tri[0][0], tri[0][-1]\n                a = ' '.join(sents[i][start_idx:end_idx+1])\n            if len(tri[1]) == 1:\n                b = sents[i][tri[1][0]]\n            else:\n                start_idx, end_idx = tri[1][0], tri[1][-1]\n                b = ' '.join(sents[i][start_idx:end_idx+1])\n            target_str += f\" {ASPECT_TOKEN} {a} {OPINION_TOKEN} {b}\"\n        targets.append(target_str.strip())\n    return inputs, targets\n\n\ndef prepare_aope_gene(args, data_type_file):\n    # just change order\n    targets, inputs = prepare_aope_universal(args, data_type_file=data_type_file)\n    return inputs, targets\n\n\ndef get_generation_inputs_and_targets(args, task, data_type):\n    if task not in [\"gene_ate\", \"gene_uabsa\", \"gene_aope\", \"gene_aste\"]:\n        raise NotImplementedError(f\"Task {task} is not supported.\")\n\n    if data_type == \"train\":\n        prepare_fn = {\n            \"gene_ate\": prepare_ate_gene,\n            \"gene_uabsa\": prepare_uabsa_gene,\n            \"gene_aope\": prepare_aope_gene,\n            \"gene_aste\": prepare_aste_gene,\n        }[task]\n        inputs, targets = prepare_fn(args, data_type_file=\"train\")\n    else:\n        raise NotImplementedError(f\"Data type {data_type} is not supported.\")\n\n    return inputs, targets\n\n\ndef get_extraction_inputs_and_targets(args, task, data_type):\n    task_mapping = {\n        \"ate\": {\"extraction\": prepare_ate_extraction, \"extraction-universal\": prepare_ate_universal},\n        \"extract_ate\": {\"extraction\": prepare_ate_extraction, \"extraction-universal\": prepare_ate_universal},\n        \"uabsa\": {\"extraction\": prepare_uabsa_extraction, \"extraction-universal\": prepare_uabsa_universal},\n        \"extract_uabsa\": {\"extraction\": prepare_uabsa_extraction, \"extraction-universal\": prepare_uabsa_universal},\n        \"aope\": {\"extraction\": prepare_uabsa_extraction, \"extraction-universal\": prepare_uabsa_universal},\n        \"extract_aope\": {\"extraction\": prepare_uabsa_extraction, \"extraction-universal\": prepare_uabsa_universal},\n        \"aste\": {\"extraction\": prepare_aste_extraction, \"extraction-universal\": prepare_aste_universal},\n        \"extract_aste\": {\"extraction\": prepare_aste_extraction, \"extraction-universal\": prepare_aste_universal},\n    }\n\n    if task not in task_mapping:\n        raise NotImplementedError\n\n    if args.paradigm not in task_mapping[task]:\n        raise NotImplementedError\n\n    inputs, targets = task_mapping[task][args.paradigm](args, data_type_file=data_type)\n\n    return inputs, targets\n\n\ndef get_inputs_and_targets(args, task, data_type):\n\n    is_gene = True if \"gene\" in task else False\n    if is_gene:\n        inputs, targets = get_generation_inputs_and_targets(args, task, data_type)\n    else:\n        inputs, targets = get_extraction_inputs_and_targets(args, task, data_type)\n        if data_type == \"train\" and task in [\"extract_ate\", \"extract_uabsa\"]:\n            # need to filter out too much none, encourage more pairs to be detected\n            inputs, targets = filter_none(inputs, targets, args.data_gene_extract_none_remove_ratio)\n\n    # rm label to prevent potential leakage\n    if data_type == \"target-unlabel\":\n        targets = [\"Label removed.\" for t in targets]\n        logger.info(\"Removed label for target-unlabel.\")\n\n    path = f\"{args.data_dir}/{data_type}_{task}_processed.txt\"\n    save_inputs_and_targets(path, inputs, targets)\n\n    return inputs, targets\n\n\ndef save_inputs_and_targets(path, inputs, targets):\n    with open(path, \"w\", encoding='utf-8') as f:\n        for i in range(len(inputs)):\n            f.write(f\"{inputs[i]} ===> {targets[i]}\\n\")\n    logger.info(f\"{path} is written.\")\n\n\ndef get_dataset(args, task, data_type, tokenizer):\n    inputs, targets = get_inputs_and_targets(args, task, data_type)\n    dataset = ABSADataset(args, tokenizer, inputs=inputs, targets=targets, name=f\"{data_type}_{task}\")\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:51.149242Z","iopub.execute_input":"2023-11-02T15:21:51.149568Z","iopub.status.idle":"2023-11-02T15:21:51.365781Z","shell.execute_reply.started":"2023-11-02T15:21:51.149542Z","shell.execute_reply":"2023-11-02T15:21:51.364632Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# eval_utils.py\nimport json\nimport re\nimport copy\n\nimport editdistance\nimport numpy as np\n\nsentiment_word_list = ['positive', 'negative', 'neutral']\n\ndef extract_spans_extraction(task, seq, io_format):\n    if task == \"uabsa\":\n        if io_format == \"extraction\":\n            return extract_uabsa_from_extration(seq)\n        elif io_format == \"extraction-universal\":\n            return extract_uabsa_from_extraction_universal(seq, io_format)\n        else:\n            raise NotImplementedError\n    elif task == \"ate\":\n        if io_format == \"extraction\":\n            return extract_ate_from_extraction(seq)\n        elif io_format == \"extraction-universal\":\n            return extract_ate_from_extraction_universal(seq)\n        else:\n            raise NotImplementedError\n    elif task == \"aste\":\n        if io_format == \"extraction\":\n            return extract_aste_from_extraction(seq)\n        elif io_format == \"extraction-universal\":\n            return extract_aste_from_extraction_universal(seq)\n        else:\n            raise NotImplementedError\n    elif task == \"aope\":\n        if io_format == \"extraction\":\n            return extract_aope_from_extraction(seq)\n        elif io_format == \"extraction-universal\":\n            return extract_aope_from_extraction_universal(seq)\n        else:\n            raise NotImplementedError\n\n\ndef extract_ate_from_extraction(seq):\n    aps = re.findall(\"(\\()(.*?)(?=\\);?|$)\", seq)\n    pairs = []\n    for ap in aps:\n        special_token, aspect = ap[0].strip(), ap[1].strip()\n        aspect = aspect.strip()\n        pairs.append((aspect))\n    return pairs\n\n\ndef extract_ate_from_extraction_universal(seq):\n\n    aps = re.findall(\"(<aspect>)(.*?)(?=<aspect>|$)\", seq)\n    pairs = []\n    for ap in aps:\n        special_token, aspect = ap[0].strip(), ap[1].strip()\n        aspect = aspect.strip()\n        pairs.append((aspect))\n    return pairs\n\n\ndef extract_aste_from_extraction_universal(seq):\n\n    aps = re.findall(\"(<pos>|<neg>|<neu>)(.+?)<opinion>(.+?)(?=<pos>|<neg>|<neu>|$)\", seq)\n    pairs = []\n    for ap in aps:\n        senti, aspect, opinion = ap[0].strip(), ap[1].strip(), ap[2].strip()\n        senti = SPECIAL_TO_TAG[senti]\n        pairs.append((aspect, opinion, senti))\n    return pairs\n\n\ndef extract_aste_from_extraction(seq):\n    aps = re.findall(\"\\((.+?),(.+?),(.+?)\\);?\", seq)\n    pairs = []\n    for ap in aps:\n        aspect, opinion, senti = ap[0].strip(), ap[1].strip(), ap[2].strip()\n        pairs.append((aspect, opinion, senti))\n    return pairs\n\n\ndef extract_aope_from_extraction_universal(seq):\n\n    aps = re.findall(\"(<aspect>)(.+?)<opinion>(.+?)(?=<aspect>|$)\", seq)\n    pairs = []\n    for ap in aps:\n        tag, aspect, opinion = ap[0].strip(), ap[1].strip(), ap[2].strip()\n        pairs.append((aspect, opinion))\n    return pairs\n\n\ndef extract_aope_from_extraction(seq):\n    aps = re.findall(\"\\((.+?),(.+?)\\);?\", seq)\n    pairs = []\n    for ap in aps:\n        aspect, opinion = ap[0].strip(), ap[1].strip()\n        pairs.append((aspect, opinion))\n    return pairs\n\n\ndef extract_uabsa_from_extration(seq):\n    aps = re.findall(\"\\((.+?),(.+?)\\);?\", seq)\n    pairs = []\n    for ap in aps:\n        aspect, opinion = ap[0].strip(), ap[1].strip()\n        pairs.append((aspect, opinion))\n    return pairs\n\n\ndef extract_uabsa_from_extraction_universal(seq, io_format, keep_special=False):\n    \"\"\"\n        extraction-universal:  <pos> apple <pos> orange <neg> banana //   [none]\n    \"\"\"\n    \"\"\"\n    matches:\n    [('manager ', '<neg>'), (' drinks ', '<pos>'), (' appetizers ', '<pos>')]\n    \"\"\"\n    # aps = re.findall(\"(<pos>|<neg>|<neu>)(.*?)(<\\/pos>|<\\/neg>|<\\/neu>)\", seq)\n    if io_format == \"extraction-universal\":\n        aps = re.findall(\"(<pos>|<neg>|<neu>)(.*?)(?=<pos>|<neg>|<neu>|$)\", seq)\n    pairs = []\n    for ap in aps:\n        if io_format in [\"extraction-universal\"]:\n            special_token, aspect = ap[0].strip(), ap[1].strip()\n        aspect = aspect.strip()\n        if keep_special:\n            senti_tag = special_token\n        else:\n            senti_tag = SPECIAL_TO_TAG[special_token]\n        pairs.append((aspect, senti_tag))\n    return pairs\n\n\ndef recover_terms_with_editdistance(original_term, sent):\n    words = original_term.split(' ')\n    new_words = []\n    for word in words:\n        edit_dis = []\n        for token in sent:\n            edit_dis.append(editdistance.eval(word, token))\n        smallest_idx = edit_dis.index(min(edit_dis))\n        new_words.append(sent[smallest_idx])\n    new_term = ' '.join(new_words)\n    return new_term\n\n\ndef fix_preds_uabsa(all_pairs, sents):\n\n    all_new_pairs = []\n    for i, pairs in enumerate(all_pairs):\n        new_pairs = []\n        if pairs == []:\n            all_new_pairs.append(pairs)\n        else:\n            for pair in pairs:\n                # AT not in the original sentence\n                if pair[0] not in  ' '.join(sents[i]):\n                    # print('Issue')\n                    new_at = recover_terms_with_editdistance(pair[0], sents[i])\n                else:\n                    new_at = pair[0]\n\n                if pair[1] not in TAG_WORD_LIST:\n                    new_sentiment = recover_terms_with_editdistance(pair[1], TAG_WORD_LIST)\n                else:\n                    new_sentiment = pair[1]\n\n                new_pairs.append((new_at, new_sentiment))\n                # print(pair, '>>>>>', word_and_sentiment)\n                # print(all_target_pairs[i])\n            all_new_pairs.append(new_pairs)\n\n    return all_new_pairs\n\n\ndef fix_preds_ate(all_pairs, sents):\n\n    all_new_pairs = []\n    for i, pairs in enumerate(all_pairs):\n        new_pairs = []\n        if pairs == []:\n            all_new_pairs.append(pairs)\n        else:\n            for pair in pairs:\n                # AT not in the original sentence\n                if pair not in  ' '.join(sents[i]):\n                    # notice here pair alone is an aspect, no need pair[0]\n                    new_at = recover_terms_with_editdistance(pair, sents[i])\n                else:\n                    new_at = pair\n\n                new_pairs.append((new_at))\n                # print(pair, '>>>>>', word_and_sentiment)\n                # print(all_target_pairs[i])\n            all_new_pairs.append(new_pairs)\n\n    return all_new_pairs\n\n\ndef fix_preds_aope(all_pairs, sents):\n\n    all_new_pairs = []\n\n    for i, pairs in enumerate(all_pairs):\n        new_pairs = []\n        if pairs == []:\n            all_new_pairs.append(pairs)\n        else:\n            for pair in pairs:\n                #print(pair)\n                # AT not in the original sentence\n                if pair[0] not in  ' '.join(sents[i]):\n                    # print('Issue')\n                    new_at = recover_terms_with_editdistance(pair[0], sents[i])\n                else:\n                    new_at = pair[0]\n\n                # OT not in the original sentence\n                ots = pair[1].split(', ')\n                new_ot_list = []\n                for ot in ots:\n                    if ot not in ' '.join(sents[i]):\n                        # print('Issue')\n                        new_ot_list.append(recover_terms_with_editdistance(ot, sents[i]))\n                    else:\n                        new_ot_list.append(ot)\n                new_ot = ', '.join(new_ot_list)\n\n                new_pairs.append((new_at, new_ot))\n                # print(pair, '>>>>>', word_and_sentiment)\n                # print(all_target_pairs[i])\n            all_new_pairs.append(new_pairs)\n\n    return all_new_pairs\n\n\ndef fix_preds_aste(all_pairs, sents):\n\n    all_new_pairs = []\n\n    for i, pairs in enumerate(all_pairs):\n        new_pairs = []\n        if pairs == []:\n            all_new_pairs.append(pairs)\n        else:\n            for pair in pairs:\n                #two formats have different orders\n                p0, p1, p2 = pair\n                # for annotation-type\n                if p1 in sentiment_word_list:\n                    at, ott, ac = p0, p2, p1\n                    io_format = 'annotation'\n                # for extraction type\n                elif p2 in sentiment_word_list:\n                    at, ott, ac = p0, p1, p2\n                    io_format = 'extraction'\n\n                #print(pair)\n                # AT not in the original sentence\n                if at not in  ' '.join(sents[i]):\n                    # print('Issue')\n                    new_at = recover_terms_with_editdistance(at, sents[i])\n                else:\n                    new_at = at\n\n                if ac not in sentiment_word_list:\n                    new_sentiment = recover_terms_with_editdistance(ac, sentiment_word_list)\n                else:\n                    new_sentiment = ac\n\n                # OT not in the original sentence\n                ots = ott.split(', ')\n                new_ot_list = []\n                for ot in ots:\n                    if ot not in ' '.join(sents[i]):\n                        # print('Issue')\n                        new_ot_list.append(recover_terms_with_editdistance(ot, sents[i]))\n                    else:\n                        new_ot_list.append(ot)\n                new_ot = ', '.join(new_ot_list)\n                if io_format == 'extraction':\n                    new_pairs.append((new_at, new_ot, new_sentiment))\n                else:\n                    new_pairs.append((new_at, new_sentiment, new_ot))\n                # print(pair, '>>>>>', word_and_sentiment)\n                # print(all_target_pairs[i])\n            all_new_pairs.append(new_pairs)\n\n    return all_new_pairs\n\n\ndef fix_pred_with_editdistance(all_predictions, sents, task):\n    if task == \"uabsa\":\n        fixed_preds = fix_preds_uabsa(all_predictions, sents)\n    elif task == \"ate\":\n        fixed_preds = fix_preds_ate(all_predictions, sents)\n    elif task == \"aste\":\n        fixed_preds = fix_preds_aste(all_predictions, sents)\n    elif task == \"aope\":\n        fixed_preds = fix_preds_aope(all_predictions, sents)\n    else:\n        logger.info(\"*** Unimplemented Error ***\")\n        fixed_preds = all_predictions\n\n    return fixed_preds\n\n\ndef compute_f1_scores(pred_pt, gold_pt):\n    \"\"\"\n    Function to compute F1 scores with pred and gold pairs/triplets\n    The input needs to be already processed\n    \"\"\"\n    # number of true postive, gold standard, predicted aspect terms\n    n_tp, n_gold, n_pred = 0, 0, 0\n    gold_pt = copy.deepcopy(gold_pt)\n\n    for i in range(len(pred_pt)):\n        n_gold += len(gold_pt[i])\n        n_pred += len(pred_pt[i])\n\n        for t in pred_pt[i]:\n            if t in gold_pt[i]:\n                # to prevent generate same correct answer and get recall larger than 1\n                gold_pt[i].remove(t)\n                n_tp += 1\n\n    precision = float(n_tp) / float(n_pred) if n_pred != 0 else 0\n    recall = float(n_tp) / float(n_gold) if n_gold != 0 else 0\n    f1 = 2 * precision * recall / (precision + recall) if precision != 0 or recall != 0 else 0\n    scores = {'precision': precision, 'recall': recall, 'f1': f1}\n\n    return scores\n\n\ndef compute_scores(pred_seqs, gold_seqs, sents, paradigm, task, verbose=False):\n    \"\"\"\n    compute metrics for multiple tasks\n    \"\"\"\n    assert len(pred_seqs) == len(gold_seqs)\n    num_samples = len(gold_seqs)\n\n    all_labels, all_predictions = [], []\n\n    for i in range(num_samples):\n        if \"extraction\" in paradigm:\n            gold_list = extract_spans_extraction(task, gold_seqs[i], paradigm)\n            pred_list = extract_spans_extraction(task, pred_seqs[i], paradigm)\n\n            all_labels.append(gold_list)\n            all_predictions.append(pred_list)\n\n    raw_scores = compute_f1_scores(all_predictions, all_labels)\n    # fix the issues due to generation\n    all_predictions_fixed = fix_pred_with_editdistance(all_predictions, sents, task)\n    fixed_scores = compute_f1_scores(all_predictions_fixed, all_labels)\n\n    if verbose:\n        for i in range(3):\n            logger.info(f\"Gold: {gold_seqs[i]}\")\n            logger.info(f\"Gold list: {all_labels[i]}\")\n            logger.info(f\"Pred: {pred_seqs[i]}\")\n            logger.info(f\"Pred list: {all_predictions[i]}\")\n        logger.info(\"Results of raw output\")\n        logger.info(str(raw_scores))\n        logger.info(\"Results of fixed output\")\n        logger.info(str(fixed_scores))\n\n    return raw_scores, fixed_scores, all_labels, all_predictions, all_predictions_fixed\n\n\ndef avg_n_seeds_by_pair(output_dir, dirs, decode_txt, n_runs):\n    score_avg_dict = {}\n    score_type_list = [\"raw_scores\", \"fixed_scores\"]\n    metric_list = [\"precision\", \"recall\", \"f1\"]\n    pairs = []\n\n    # collect value\n    for dir_ in dirs:\n        for score_type in score_type_list:\n            if score_type not in score_avg_dict:\n                score_avg_dict[score_type] = {}\n            pair = dir_.split('/')[-1]\n            src, tgt = pair.split('-')\n            if pair not in score_avg_dict[score_type]:\n                score_avg_dict[score_type][pair] = {}\n            score_dict_i = json.load(open(f\"{dir_}/score/test_{decode_txt}_score.json\",\"r\", encoding='UTF-8'))\n\n            for metric in metric_list:\n                if metric not in score_avg_dict[score_type][pair]:\n                    score_avg_dict[score_type][pair][metric] = []\n                score_avg_dict[score_type][pair][metric].append(score_dict_i[score_type][tgt][metric])\n\n    # get all value\n    for score_type in score_type_list:\n        all_mat_dict = {k: [] for k in metric_list}\n        for pair in score_avg_dict[score_type]:\n            for metric in metric_list:\n                f1_list_by_seed = score_avg_dict[score_type][pair][metric]\n                all_mat_dict[metric].append(f1_list_by_seed)\n        for metric in metric_list:\n            if \"all\" not in score_avg_dict[score_type]:\n                score_avg_dict[score_type][\"all\"] = {}\n            score_avg_dict[score_type][\"all\"][metric] = np.mean(all_mat_dict[metric], axis=0)\n\n    # avg value\n    for score_type in score_type_list:\n        for pair in score_avg_dict[score_type]:\n            for metric in metric_list:\n                mean = np.mean(score_avg_dict[score_type][pair][metric])\n                std = np.std(score_avg_dict[score_type][pair][metric])\n                score_avg_dict[score_type][pair][metric] = (mean, std)\n\n    # visual result\n    for score_type in score_type_list:\n        logger.info('@'*100)\n        logger.info(f\"Avged {n_runs} runs {score_type}\")\n        logger.info('\\t'.join(list(score_avg_dict[score_type].keys())))\n        f1_list = [i[\"f1\"][0] for i in list(score_avg_dict[score_type].values())]\n        logger.info('\\t'.join([f\"{i*100:.2f}\" for i in f1_list]))\n        f1_std_list = [i[\"f1\"][1] for i in list(score_avg_dict[score_type].values())]\n        logger.info('\\t'.join([f\"{i*100:.2f}\" for i in f1_std_list]))\n\n    json.dump(score_avg_dict, open(output_dir+f\"/score_{decode_txt}_avg.json\", \"w\", encoding='UTF-8'), indent=2)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:51.367900Z","iopub.execute_input":"2023-11-02T15:21:51.368390Z","iopub.status.idle":"2023-11-02T15:21:51.442571Z","shell.execute_reply.started":"2023-11-02T15:21:51.368351Z","shell.execute_reply":"2023-11-02T15:21:51.441329Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# preprocess.py\nimport os\nimport re\n\ndef ot2bieos_absa(absa_tag_sequence):\n    \"\"\"\n    ot2bieos function for end-to-end aspect-based sentiment analysis task\n    \"\"\"\n    n_tags = len(absa_tag_sequence)\n    #new_ts_sequence = []\n    new_absa_sequence = []\n    prev_pos = '$$$'\n\n    for i in range(n_tags):\n        cur_absa_tag = absa_tag_sequence[i]\n        if cur_absa_tag == 'O' or cur_absa_tag == 'EQ':\n            # when meet the EQ tag, regard it as O\n            new_absa_sequence.append('O')\n            cur_pos = 'O'\n        else:\n            cur_pos, cur_sentiment = cur_absa_tag.split('-')\n            # cur_pos is T\n            if cur_pos != prev_pos:\n                # prev_pos is O and new_cur_pos can only be B or S\n                if i == n_tags - 1:\n                    new_absa_sequence.append('S-%s' % cur_sentiment)\n                else:\n                    next_absa_tag = absa_tag_sequence[i + 1]\n                    if next_absa_tag == 'O':\n                        new_absa_sequence.append('S-%s' % cur_sentiment)\n                    else:\n                        new_absa_sequence.append('B-%s' % cur_sentiment)\n            else:\n                # prev_pos is T and new_cur_pos can only be I or E\n                if i == n_tags - 1:\n                    new_absa_sequence.append('E-%s' % cur_sentiment)\n                else:\n                    next_absa_tag = absa_tag_sequence[i + 1]\n                    if next_absa_tag == 'O':\n                        new_absa_sequence.append('E-%s' % cur_sentiment)\n                    else:\n                        new_absa_sequence.append('I-%s' % cur_sentiment)\n        prev_pos = cur_pos\n    return new_absa_sequence\n\n\ndef bieos2generation(sents, labels):\n    final_sents = []\n\n    for si, s in enumerate(sents):\n        pairs = []\n        aspect_idx = []\n        for wi, w in enumerate(s):\n            tag = labels[si][wi]\n            if tag == \"O\":\n                aspect_idx = []\n                continue\n\n            label, polarity = labels[si][wi].split('-')\n            if label in [\"B\", \"I\"]:\n                aspect_idx.append(wi)\n            elif label in [\"E\", \"S\"]:\n                aspect_idx.append(wi)\n                aspect_tuple = (aspect_idx, polarity)\n                pairs.append(aspect_tuple)\n                aspect_idx = []\n\n        final_s = ' '.join(s) + \"#\"*4 + str(pairs)\n        final_sents.append(final_s)\n\n    return final_sents\n\n\ndef read_generation_uabsa(file_path):\n    # [\"I love apple .####[([0, \"POS\"])]\"]\n    sents, labels = read_by_bieos(file_path)\n    final_sents = bieos2generation(sents, labels)\n    return final_sents\n\n\ndef read_by_bieos(file_path):\n    sents, labels  = [], []\n    with open(file_path, 'r', encoding='UTF-8') as fp:\n        words, tags = [], []\n        for line in fp:\n            word_part, label_part = line.strip().split(\"####\")\n            # I=O love=O apple=T-POS\n            tokens = label_part.split(\" \")\n            # remove some period \".\"\n            tokens = [t for t in tokens if \"=\" in t]\n            # sometimes there are multiple =, such as ==O\n            words = [\"\".join(i.split(\"=\")[:-1]) for i in tokens]\n            tags = [i.split(\"=\")[-1] for i in tokens]\n\n            tags = ot2bieos_absa(tags)\n            sents.append(words)\n            labels.append(tags)\n            words, tags = [], []\n    return sents, labels\n\n\ndef write_generation(file_paths, split=\"train\", data_dir=None, do_write=True, nrows=None, mode='aste'):\n\n    final_sents = []\n    count_dict = {}\n    for path in file_paths:\n\n        sentences, domain = None, None\n        if mode == 'aste':\n            sentences = open(path, \"r\", encoding='UTF-8').readlines()[:nrows]\n            if split in [\"dev\", \"test\"]:\n                domain = path.split('/')[-2]\n        elif mode == \"uabsa\":  # 'uabsa'\n            sentences = read_generation_uabsa(path)[:nrows]\n            if split in [\"dev\", \"test\"]:\n                domain = re.search(f\"(\\w+)[_-]{split}\\.txt\", path).group(1)\n\n        count_dict[domain] = len(sentences)\n        final_sents.extend(sentences)\n        logger.info(f\"{split} {path}: {len(sentences)}\")\n\n    logger.info(f\"{split} total: {len(final_sents)}\")\n    logger.info(f\"{split} count dict: {count_dict}\")\n\n    if do_write:\n        output_dir = data_dir\n\n        if not os.path.exists(output_dir):\n            os.mkdir(output_dir)\n\n        output_path = f'{output_dir}/{split}.txt'\n        with open (output_path, 'w', encoding='utf-8') as f:\n            for s in final_sents:\n                f.write (f'{s.strip()}\\n')\n            logger.info(f\"{output_path} is written.\")\n\n    return final_sents, count_dict\n\n\ndef preprocess(dataset_dir=None, data_dir=None, source=None, targets=None, do_write=True, nrows=None, unlabel=False, mode='uabsa'):\n\n    if mode == 'uabsa':\n        train_file_format = \"{i}_train.txt\"\n        dev_file_format = f\"{source}_dev.txt\"\n        test_file_format = \"{i}_test.txt\"\n    elif mode == 'aste':\n        train_file_format = \"{i}/train.txt\"\n        dev_file_format = f\"{source}/dev.txt\"\n        test_file_format = \"{i}/test.txt\"\n    else:\n        raise ValueError(\"Invalid mode. Choose either 'uabsa' or 'aste'.\")\n\n    if unlabel:\n        # Process unlabeled data\n        train_paths = [os.path.join(dataset_dir, train_file_format.format(i=i)) for i in targets]\n        train_sents, _ = write_generation(train_paths, \"target-unlabel\", data_dir=data_dir, do_write=do_write, nrows=nrows, mode=mode)\n        logger.info(f\"Cross domain unlabel train_paths: {train_paths}\")\n        return train_sents\n\n    else:\n        # Process labeled data\n        train_paths = [os.path.join(dataset_dir, train_file_format.format(i=i)) for i in [source]]\n        dev_paths = [os.path.join(dataset_dir, dev_file_format)]\n        test_paths = [os.path.join(dataset_dir, test_file_format.format(i=i)) for i in targets]\n\n        logger.info(f\"train_paths: {train_paths}\")\n        logger.info(f\"dev_paths: {dev_paths}\")\n        logger.info(f\"test_paths: {test_paths}\")\n\n        # Read and preprocess the data using write_generation\n        _, _ = write_generation(train_paths, \"train\", data_dir=data_dir, do_write=do_write, nrows=nrows, mode=mode)\n        _, dev_count_dict = write_generation(dev_paths, \"dev\", data_dir=data_dir, do_write=do_write, nrows=nrows, mode=mode)\n        _, test_count_dict = write_generation(test_paths, \"test\", data_dir=data_dir, do_write=do_write, nrows=nrows, mode=mode)\n\n        return dev_count_dict, test_count_dict\n\n\ndef prepare_raw_data(args):\n\n    def process_task(preprocessor, mode, dataset_dir=None, data_gene=False, pseudo=False):\n        eval_count_dict, test_count_dict = preprocessor(\n            dataset_dir=dataset_dir,\n            data_dir=args.data_dir,\n            source=args.source_domain,\n            targets=args.target_domain,\n            nrows=args.nrows,\n            mode=mode\n        )\n        if data_gene or pseudo:\n            preprocessor(\n                dataset_dir=dataset_dir,\n                data_dir=args.data_dir,\n                source=args.source_domain,\n                targets=args.target_domain,\n                nrows=args.nrows,\n                unlabel=True,\n                mode=mode\n            )\n        return eval_count_dict, test_count_dict\n\n    # ate and uabsa share same dataset\n    task_map = {\n        \"uabsa\": ('uabsa',  \"/kaggle/input/bgca-ko/uabsa/cross_domain\"),\n        \"ate\": ('uabsa',  \"/kaggle/input/bgca-ko/uabsa/cross_domain\"),\n        \"aste\": ('aste', \"/kaggle/input/bgca-ko/aste/cross_domain\"),\n        \"aope\": ('aste', \"/kaggle/input/bgca-ko/aope/cross_domain\")\n    }\n\n    if args.task in task_map and args.dataset == \"cross_domain\":\n        mode, dataset_dir = task_map[args.task]\n        return process_task(preprocess, mode, dataset_dir, args.data_gene, args.pseudo)\n    else:\n        raise NotImplementedError\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:51.445891Z","iopub.execute_input":"2023-11-02T15:21:51.446400Z","iopub.status.idle":"2023-11-02T15:21:51.486324Z","shell.execute_reply.started":"2023-11-02T15:21:51.446360Z","shell.execute_reply":"2023-11-02T15:21:51.485168Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# model_utils.py\nimport torch\n\ndef prepare_constrained_tokens(tokenizer, task, paradigm):\n    special_tokens = [tokenizer.eos_token] # add end token\n    if task == \"uabsa\":\n        if paradigm == \"annotation\":\n            # note space will affect tokenization, make sure the special tokens align with processed method considering space\n            special_tokens += [\"[a|negative]\", \"[a|positive]\", \"[a|neutral]\"]\n        elif paradigm == \"extraction\":\n            # (service, positive); (resturant, positive)\n            special_tokens += [\"( a , neutral ) ; ( a , negative ) ; ( a , positive ) \"]\n            special_tokens += [\"None\"]\n        elif paradigm == \"extraction-universal\":\n            special_tokens += [i[0] for i in TAG_TO_SPECIAL.values()]\n            special_tokens += [NONE_TOKEN]\n        else:\n            raise NotImplementedError\n    elif task == \"ate\":\n        if paradigm == \"annotation\":\n            special_tokens += [\"[aspect]\"]\n        elif paradigm == \"extraction\":\n            # (service, positive); (resturant, positive)\n            special_tokens += [\"( a ) ; ( a ) ;\"]\n            special_tokens += [\"None\"]\n        elif paradigm == \"extraction-universal\":\n            special_tokens += [ASPECT_TOKEN]\n            special_tokens += [NONE_TOKEN]\n        else:\n            raise NotImplementedError\n    elif task == \"aste\":\n        if paradigm == \"extraction-universal\":\n            special_tokens += [i[0] for i in TAG_TO_SPECIAL.values()]\n            special_tokens += [OPINION_TOKEN]\n        elif paradigm == \"extraction\":\n            special_tokens += [\"( a , a , negative ) ; ( a , a , neutral ) ; ( a , a , positive )\"]\n        else:\n            raise NotImplementedError\n    elif task == \"aope\":\n        if paradigm == \"extraction-universal\":\n            special_tokens += [ASPECT_TOKEN, OPINION_TOKEN, SEP_TOKEN]\n        elif paradigm == \"extraction\":\n            special_tokens += [\"( a , a ) ; ( a , a )\"]\n        else:\n            raise NotImplementedError\n    else:\n        raise NotImplementedError\n    return special_tokens\n\n\ndef prepare_tag_tokens(args):\n    tag_tokens = []\n    if args.task == \"uabsa\":\n        if \"extraction-universal\" in args.paradigm:\n            tag_tokens += [i[0] for i in TAG_TO_SPECIAL.values()]\n            tag_tokens += [NONE_TOKEN]\n        if args.data_gene:\n            tag_tokens += [j for i in TAG_TO_SPECIAL.values() for j in i]\n            tag_tokens += [ASPECT_TOKEN, OPINION_TOKEN, EMPTY_TOKEN, SEP_TOKEN]\n    elif args.task == \"ate\":\n        if args.paradigm == \"extraction-universal\":\n            tag_tokens += [ASPECT_TOKEN, NONE_TOKEN, SEP_TOKEN]\n    elif args.task == \"aste\":\n        if \"extraction-universal\" in args.paradigm:\n            tag_tokens += [i[0] for i in TAG_TO_SPECIAL.values()]\n            tag_tokens += [OPINION_TOKEN, SEP_TOKEN]\n    elif args.task == \"aope\":\n        if \"extraction-universal\" in args.paradigm:\n            tag_tokens += [ASPECT_TOKEN, OPINION_TOKEN, SEP_TOKEN]\n    else:\n        raise NotImplementedError\n\n    tag_tokens = list(set(tag_tokens))\n    logger.info(f\"Tag tokens: {tag_tokens}\")\n    return tag_tokens\n\n\ndef init_tag(args, tokenizer, model, tag_tokens):\n    if args.init_tag == \"english\":\n        if args.paradigm == \"extraction-universal\":\n            import re\n            map_dict = {\"pos\": \"positive\", \"neg\": \"negative\", \"neu\": \"neutral\"}\n            for tag_word in tag_tokens:\n                tag_id = tokenizer.encode(tag_word, add_special_tokens=False)[0]\n                init_word = re.sub(\"\\W\", \"\", tag_word).strip()\n                # map senti\n                if init_word in map_dict:\n                    init_word = map_dict[init_word]\n                # skip sep\n                elif init_word == \"sep\":\n                    continue\n                init_id = tokenizer.encode(init_word, add_special_tokens=False)[0]\n                with torch.no_grad():\n                    model.shared.weight[tag_id] = model.shared.weight[init_id]\n                logger.info(f\"{tokenizer.decode(tag_id)} is init by {tokenizer.decode(init_id)}\")\n        elif args.paradigm == \"extraction\":\n            pass\n        else:\n            raise NotImplementedError\n    else:\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:51.487996Z","iopub.execute_input":"2023-11-02T15:21:51.488925Z","iopub.status.idle":"2023-11-02T15:21:51.514659Z","shell.execute_reply.started":"2023-11-02T15:21:51.488883Z","shell.execute_reply":"2023-11-02T15:21:51.513515Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# run_utils.py\nimport re\nimport os\nimport random\n\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\n# from tqdm import tqdm, trange\nfrom tqdm.notebook import tqdm, trange\nfrom transformers import (AutoModelForSeq2SeqLM,\n                          T5Tokenizer, get_linear_schedule_with_warmup)\n\nclass Prefix_fn_cls():\n    def __init__(self, tokenizer, special_tokens, input_enc_idxs):\n        self.tokenizer=tokenizer\n        self.input_enc_idxs=input_enc_idxs\n        self.special_ids = [element for l in self.tokenizer(special_tokens, add_special_tokens=False)['input_ids'] for element in l]\n        self.special_ids = list(set(self.special_ids))\n\n    def get(self, batch_id, previous_tokens):\n        # get input\n        inputs = list(set(self.input_enc_idxs[batch_id].tolist()))+self.special_ids\n        return inputs\n\n\ndef train(args, tokenizer, model, train_dataset, task, epochs, lr, bs, acc_step=None, save_ckpt=False, save_last=False):\n    start_info = \"#\"*20+f\" Conduct {task} Training\"+\"#\"*20\n    logger.info(\"#\"*len(start_info))\n    logger.info(start_info)\n    logger.info(\"#\"*len(start_info))\n\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        { \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay, },\n        { \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=args.adam_epsilon)\n\n    if acc_step is None:\n        acc_step = args.gradient_accumulation_steps\n\n    train_dataloader = DataLoader(train_dataset, batch_size=bs, drop_last=True, shuffle=True, num_workers=4)\n    t_total = (\n        (len(train_dataloader.dataset) // (bs * max(1, args.n_gpu)))\n        // acc_step\n        * float(epochs)\n    )\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    train_iterator = trange(int(epochs), dynamic_ncols=True, desc=\"Epoch\")\n\n    # visualize input\n    logger.info(f\"Training examples out of {len(train_dataset)}:\")\n    for i in range(3):\n        logger.info(f'Input : {tokenizer.decode(train_dataset[i][\"source_ids\"], skip_special_tokens=True)}')\n        logger.info(f'Output: {tokenizer.decode(train_dataset[i][\"target_ids\"], skip_special_tokens=True)}')\n\n    logger.info(f\"Model emb weights of <pad> {model.shared.weight[0][:5]}\")\n    # start training\n    for n_epoch, _ in enumerate(train_iterator):\n        epoch_train_loss = 0.0\n        epoch_iterator = tqdm(train_dataloader, dynamic_ncols=True, desc=\"Iteration\")\n        for step, batch in enumerate(epoch_iterator):\n            model.train()\n\n            lm_labels = batch[\"target_ids\"]\n            lm_labels[lm_labels[:, :] == tokenizer.pad_token_id] = -100\n\n            outputs = model(\n                batch[\"source_ids\"].to(args.device),\n                attention_mask=batch[\"source_mask\"].to(args.device),\n                labels=lm_labels.to(args.device),\n                decoder_attention_mask=batch['target_mask'].to(args.device),\n                decoder_input_ids=None,\n            )\n\n            loss = outputs[0]\n            loss.backward()\n            epoch_train_loss += loss.item()\n\n            if (step+1) % acc_step == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n\n        if save_ckpt and n_epoch in range(args.num_train_epochs)[-args.save_last_k:]:\n            ckpt_dir = os.path.join(args.seed_dir, f\"checkpoint-e{n_epoch}\")\n            if not os.path.exists(ckpt_dir):\n                os.makedirs(ckpt_dir)\n            model.save_pretrained(ckpt_dir) # Output 용량 한계\n            tokenizer.save_pretrained(ckpt_dir)\n            # json.dump(args, open(f\"{ckpt_dir}/args.json\", 'w', encoding='UTF-8'), indent=4)\n            logger.info(f\"Save model checkpoint to {ckpt_dir}\")\n\n        logger.info(f\"Epoch {n_epoch} Avg epoch train loss: {epoch_train_loss / len(epoch_iterator):.5f} lr: {scheduler.get_last_lr()}\")\n\n    if save_last:\n        ckpt_dir = os.path.join(args.seed_dir, f\"{task}-model\")\n        if not os.path.exists(ckpt_dir):\n            os.makedirs(ckpt_dir)\n        model.save_pretrained(ckpt_dir)\n        tokenizer.save_pretrained(ckpt_dir)\n        logger.info(f\"Save model checkpoint to {ckpt_dir}\")\n\n    logger.info(\"Finish training!\")\n\n\ndef aux_training(args, tokenizer, model, train_dataset):\n\n    return_values = {}\n    if args.data_gene:\n        return_values[\"data_gene_dataset\"] = data_gene(args, tokenizer, model, train_dataset)\n    if args.pseudo:\n        return_values[\"pseudo_dataset\"] = pseudo_label(args, tokenizer, model, train_dataset)\n\n    return return_values\n\n\ndef infer(args, dataset, model, tokenizer, name, is_constrained=False, constrained_vocab=None, keep_mask=False, **decode_dict):\n    dataloader = DataLoader(dataset, batch_size=args.eval_batch_size, num_workers=4)\n\n    if keep_mask:\n        # can't skip special directly, will lose extra_id\n        unwanted_tokens = [tokenizer.eos_token, tokenizer.unk_token, tokenizer.pad_token]\n        unwanted_ids = tokenizer.convert_tokens_to_ids(unwanted_tokens)\n        def filter_decode(ids):\n            ids = [i for i in ids if i not in unwanted_ids]\n            tokens = tokenizer.convert_ids_to_tokens(ids)\n            sentence = tokenizer.convert_tokens_to_string(tokens)\n            return sentence\n\n    # inference\n    inputs, outputs, targets = [], [], []\n    logger.info(f\"Inferencing on {name} ...\")\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            if is_constrained:\n                prefix_fn_obj = Prefix_fn_cls(tokenizer, constrained_vocab, batch['source_ids'].to(args.device))\n                prefix_fn = lambda batch_id, sent: prefix_fn_obj.get(batch_id, sent)\n            else:\n                prefix_fn = None\n\n            outs_dict = model.generate(input_ids=batch['source_ids'].to(args.device),\n                                        attention_mask=batch['source_mask'].to(args.device),\n                                        max_length=128,\n                                        prefix_allowed_tokens_fn=prefix_fn,\n                                        output_scores=True,\n                                        return_dict_in_generate=True,\n                                        **decode_dict,\n                                        )\n            outs = outs_dict[\"sequences\"]\n\n            if keep_mask:\n                input_ = [filter_decode(ids) for ids in batch[\"source_ids\"]]\n                dec = [filter_decode(ids) for ids in outs]\n                target = [filter_decode(ids) for ids in batch[\"target_ids\"]]\n            else:\n                input_ = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"source_ids\"]]\n                dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n                target = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"target_ids\"]]\n\n            inputs.extend(input_)\n            outputs.extend(dec)\n            targets.extend(target)\n\n    decode_txt = \"constrained\" if is_constrained else \"greedy\"\n    with open(os.path.join(args.inference_dir, f\"{name}_{decode_txt}_output.txt\"), \"w\", encoding='utf-8') as f:\n        for i, o in enumerate(outputs):\n            f.write(f\"{inputs[i]} ===> {o}\\n\")\n\n    return inputs, outputs, targets\n\n\ndef data_gene(args, tokenizer, model, train_dataset):\n\n    # 1. extract label\n    if args.data_gene_extract:\n        extract_task = f\"extract_{args.task}\"\n        target_extract_inputs, target_extract_outputs = extract_model(args, tokenizer, model, extract_task)\n\n    # 2. gene train & infer\n    target_gene_aug_inputs, target_gene_aug_outputs = gene_model(\n        args, tokenizer, model, target_extract_inputs, target_extract_outputs\n    )\n\n    # 3. postprocess\n    # change direction & post process\n    target_gene_aug_inputs_processed, target_gene_aug_targets_processed = postprocess_gene_outputs(args, target_gene_aug_inputs, target_gene_aug_outputs)\n\n    # 3.1 use the extract model to do filtering\n    if args.model_filter:\n        target_gene_aug_inputs_processed, target_gene_aug_targets_processed = model_filter(args, target_gene_aug_inputs_processed, target_gene_aug_targets_processed)\n        logger.info(f\"Aug num after filtering: {len(target_gene_aug_inputs_processed)}\")\n\n    # 3.2 control number of augmentation number\n    if args.data_gene_aug_num:\n        target_gene_aug_inputs_processed, target_gene_aug_targets_processed = \\\n            target_gene_aug_inputs_processed[:args.data_gene_aug_num], target_gene_aug_targets_processed[:args.data_gene_aug_num]\n    if args.data_gene_aug_ratio:\n        aug_num = int(len(target_gene_aug_inputs_processed) * args.data_gene_aug_ratio)\n        target_gene_aug_inputs_processed, target_gene_aug_targets_processed = \\\n            target_gene_aug_inputs_processed[:aug_num], target_gene_aug_targets_processed[:aug_num]\n\n    logger.info(f\"Aug num final: {len(target_gene_aug_inputs_processed)}\")\n\n    # 4. merge dataset\n    target_gene_aug_dataset = ABSADataset(args, tokenizer, inputs=target_gene_aug_inputs_processed, targets=target_gene_aug_targets_processed, name=\"target_gene_aug\")\n    train_dataset_merged = ABSADataset(args, tokenizer, dataset_list=[train_dataset, target_gene_aug_dataset])\n\n    return train_dataset_merged\n\n\ndef pseudo_label(args, tokenizer, model, train_dataset):\n    # 1. train absa on train\n    train(args, tokenizer, model, train_dataset, task=f\"pseudo_{args.task}\", epochs=args.num_train_epochs, lr=args.learning_rate, bs=args.train_batch_size, save_ckpt=False, save_last=True)\n\n    # 2. inference on target unlabel\n    target_dataset = get_dataset(args, task=args.task, data_type=\"target-unlabel\", tokenizer=tokenizer)\n    target_pseudo_inputs, target_pseudo_outputs, _ = infer(\n        args, target_dataset, model, tokenizer, name=f\"target_pseudo_{args.task}\",\n        is_constrained=True, constrained_vocab=prepare_constrained_tokens(tokenizer, args.task, args.paradigm),\n    )\n\n    if args.pseudo_skip_none:\n        target_pseudo_inputs, target_pseudo_outputs = pseudo_filter_none(target_pseudo_inputs, target_pseudo_outputs)\n\n    # 3. merge pseudo labelled data\n    target_pseudo_aug_dataset = ABSADataset(args, tokenizer, inputs=target_pseudo_inputs, targets=target_pseudo_outputs, name=\"target_pseudo_absa\")\n    train_dataset_merged = ABSADataset(args, tokenizer, dataset_list=[train_dataset, target_pseudo_aug_dataset])\n    return train_dataset_merged\n\n\ndef pseudo_filter_none(inputs, outputs):\n    new_inputs, new_outputs = [], []\n    for idx in range(len(outputs)):\n        if \"none\" not in outputs[idx]:\n            new_inputs.append(inputs[idx])\n            new_outputs.append(outputs[idx])\n    return new_inputs, new_outputs\n\n\ndef extract_model(args, tokenizer, model, extract_task):\n\n    # 1. train extract model\n    if args.extract_model:\n        model = AutoModelForSeq2SeqLM.from_pretrained(args.extract_model).to(args.device)\n        tokenizer = T5Tokenizer.from_pretrained(args.extract_model, legacy=False)\n        logger.info(f\"Model reloaded with {args.extract_model}\")\n        logger.info(f\"Tokenizer len: {len(tokenizer)}\")\n    elif args.runned_folder:\n        model_path = os.path.join(args.runned_folder, f\"seed-{args.seed}\",\n        f\"{args.source_domain}-{args.target_domain[0]}\", f\"extract_{args.task}-model\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(args.device)\n        tokenizer = T5Tokenizer.from_pretrained(model_path, legacy=False)\n        logger.info(f\"Model reloaded with {model_path}\")\n\n    train_extract_dataset = get_dataset(args, task=extract_task, data_type=\"train\", tokenizer=tokenizer)\n    train(args, tokenizer, model, train_extract_dataset, task=extract_task, epochs=args.data_gene_extract_epochs, lr=args.learning_rate, bs=args.train_batch_size, save_ckpt=False, save_last=True)\n\n    # 2. infer on target domain\n    target_extract_dataset = get_dataset(args, task=extract_task, data_type=\"target-unlabel\", tokenizer=tokenizer)\n    target_extract_inputs, target_extract_outputs, _ = infer(\n        args, target_extract_dataset, model, tokenizer,\n        name=f\"target_{extract_task}\", is_constrained=True, constrained_vocab=prepare_tag_tokens(args)\n    )\n    return target_extract_inputs, target_extract_outputs\n\n\ndef gene_model(args, tokenizer, model, target_extract_inputs, target_extract_outputs):\n\n    if args.gene_model:\n        model = AutoModelForSeq2SeqLM.from_pretrained(args.gene_model).to(args.device)\n        tokenizer = T5Tokenizer.from_pretrained(args.gene_model, legacy=False)\n        logger.info(f\"Model reloaded with {args.gene_model}\")\n    elif args.runned_folder:\n        model_path = os.path.join(args.runned_folder, f\"seed-{args.seed}\",\n        f\"{args.source_domain}-{args.target_domain[0]}\", f\"gene_{args.task}-model\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(args.device)\n        tokenizer = T5Tokenizer.from_pretrained(model_path, legacy=False)\n        logger.info(f\"Model reloaded with {model_path}\")\n    # 0. load a new model\n    elif args.data_gene_same_model or args.use_same_model:\n        logger.info(f\"Model keep the same.\")\n    else:\n        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path).to(args.device)\n        model.resize_token_embeddings(len(tokenizer))\n        logger.info(f\"Model reloaded with {args.model_name_or_path}\")\n    logger.info(f\"Tokenizer len: {len(tokenizer)}\")\n\n    # 1. train gene model\n    train_gene_dataset = get_dataset(args, task=f\"gene_{args.task}\", data_type=\"train\", tokenizer=tokenizer)\n    train(args, tokenizer, model, train_gene_dataset, task=f\"gene_{args.task}\", epochs=args.data_gene_epochs, lr=args.learning_rate, bs=args.train_batch_size, save_ckpt=False, save_last=True)\n\n    # 2. infer gene model\n    # 2.0 prepare infer dataset\n    if args.data_gene_extract:\n        target_gene_inputs, target_gene_targets = target_extract_outputs, target_extract_inputs\n\n        # ate, uabsa may contain [none] token, need to append rand word to generate diverse output\n        for idx in range(len(target_gene_inputs)):\n            if args.data_gene_none_word_num > 0 and NONE_TOKEN in target_gene_inputs[idx]:\n                add_rand = False\n                if args.task in [\"ate\", \"uabsa\"]:\n                    add_rand = True\n                if add_rand:\n                    words = target_gene_targets[idx].split()\n                    sample_num = min(len(words), args.data_gene_none_word_num)\n                    random_words = \" \".join(random.sample(words, sample_num))\n                    target_gene_inputs[idx] += \" \" + random_words\n\n        target_gene_dataset = ABSADataset(args, tokenizer, inputs=target_gene_inputs, targets=target_gene_targets, name=\"target_gene\")\n\n    # 2.1 constrained decoding, but may not be used depends on args.data_gene_wt_constrained\n    target_domain_words = prepare_gene_vocab(args)\n\n    # 2.2 inference\n    decode_dict = {\"min_length\": args.data_gene_min_length,}\n    specific_dict = {\n        \"greedy\": {\"do_sample\": False},\n        \"top_p\": {\"do_sample\": True, \"top_p\": args.data_gene_top_p},\n        \"beam\": {\"num_beams\": args.data_gene_num_beam, \"early_stopping\": True},\n    }\n    if args.data_gene_decode:\n        decode_dict.update(specific_dict[args.data_gene_decode])\n\n    is_constrained = False if args.data_gene_wt_constrained else True\n    target_gene_aug_inputs, target_gene_aug_outputs, _ = infer(\n        args, target_gene_dataset, model, tokenizer, name=\"target_gene\",\n        is_constrained=is_constrained, constrained_vocab=target_domain_words, **decode_dict\n    )\n\n    return target_gene_aug_inputs, target_gene_aug_outputs\n\n\ndef postprocess_gene_outputs(args, target_gene_aug_inputs, target_gene_aug_outputs):\n    # process input & output\n    target_gene_aug_inputs_processed = target_gene_aug_outputs\n    target_gene_aug_targets_processed = [i.strip() for i in target_gene_aug_inputs]\n\n    # normalize and filter\n    target_gene_aug_inputs_processed, target_gene_aug_targets_processed = normalize_augment(args, target_gene_aug_inputs_processed, target_gene_aug_targets_processed)\n    target_gene_aug_inputs_processed, target_gene_aug_targets_processed = filter_none(target_gene_aug_inputs_processed, target_gene_aug_targets_processed, args.data_gene_none_remove_ratio)\n    target_gene_aug_inputs_processed, target_gene_aug_targets_processed = filter_invalid(target_gene_aug_inputs_processed, target_gene_aug_targets_processed)\n\n    return target_gene_aug_inputs_processed, target_gene_aug_targets_processed\n\n\ndef extract_label_words(inputs):\n    label_words = []\n    for i in inputs:\n        i = re.sub(\"<\\w+>\", \"\", i)\n        label_words += i.strip().split()\n    return label_words\n\n\ndef prepare_gene_vocab(args):\n    # Get target inputs\n    target_inputs = get_inputs(args, data_type_file=\"target-unlabel\")\n    # Create a set to avoid duplicates and improve performance\n    target_domain_words = set(\" \".join(target_inputs).split())\n    # Extend with tag tokens\n    target_domain_words.update(prepare_tag_tokens(args))\n    logger.info(f\"{len(target_domain_words)} target domain words\")\n    target_domain_words = list(target_domain_words)\n\n    # add punctuatio words and stop words\n    import string\n    target_domain_words.extend(list(string.punctuation))\n    target_domain_words.extend(STOP_WORDS)\n\n    return target_domain_words\n\n\ndef model_filter(args, inputs, outputs):\n\n    extract_path = os.path.join(args.seed_dir, f\"extract_{args.task}-model\")\n    model2 = AutoModelForSeq2SeqLM.from_pretrained(extract_path).to(args.device)\n    tokenizer2 = T5Tokenizer.from_pretrained(extract_path, legacy=False)\n    logger.info(f\"{extract_path} loaded.\")\n    logger.info(f\"Model emb weights of <pad> {model2.shared.weight[0][:5]}\")\n\n    filter_dataset = ABSADataset(args, tokenizer2, inputs=inputs, targets=outputs, name=\"target_filter\")\n    filter_inputs, filter_outputs, _  = infer(\n        args, filter_dataset, model2, tokenizer2,\n        name=f\"target_filter\", is_constrained=True, constrained_vocab=prepare_tag_tokens(args)\n    )\n\n    assert len(filter_inputs) == len(inputs)\n\n    removed = []\n\n    new_inputs, new_outputs = [], []\n    filter_num = 0\n    for i in range(len(outputs)):\n        if filter_outputs[i].strip() != outputs[i].strip():\n            # if predict none, then use generated results to allow more tgt domain exploration\n            if args.model_filter_skip_none and args.task in [\"ate\", \"uabsa\"]:\n                    if \"none\" in filter_outputs[i].strip():\n                        new_inputs.append(inputs[i])\n                        new_outputs.append(outputs[i])\n                        continue\n            filter_num += 1\n            removed.append(' #### '.join([inputs[i], outputs[i], filter_outputs[i]]))\n            continue\n        else:\n            new_inputs.append(inputs[i])\n            new_outputs.append(outputs[i])\n\n    logger.info(f\"{filter_num} augmentations out of {len(inputs)} are removed by model.\")\n\n    with open(os.path.join(args.inference_dir, f\"model_filter.txt\"), \"w\", encoding='utf-8') as f:\n        for i, o in enumerate(removed):\n            f.write(f\"{o}\\n\")\n\n    return new_inputs, new_outputs","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:21:51.516518Z","iopub.execute_input":"2023-11-02T15:21:51.517184Z","iopub.status.idle":"2023-11-02T15:22:01.249874Z","shell.execute_reply.started":"2023-11-02T15:21:51.517154Z","shell.execute_reply":"2023-11-02T15:22:01.248816Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# setup.py\nimport argparse\nimport os\nimport random\nfrom datetime import datetime\n\ndef init_args():\n    parser = argparse.ArgumentParser()\n    # basic settings\n    parser.add_argument(\"--task\", default='uabsa', type=str, required=True,\n                        help=\"The name of the task, selected from: [xabsa]\")\n    parser.add_argument(\"--device\", default='cuda', type=str, required=False)\n    parser.add_argument(\"--dataset\", default='cross_domain', type=str, required=True,\n                        help=\"The name of the dataset, selected from: [cross_domain]\")\n    parser.add_argument(\"--data_dir\", default=None, type=str, help=\"args.output_dir/data\")\n    parser.add_argument(\"--model_name_or_path\", default='mt5-base', type=str,\n                        help=\"Path to pre-trained model or shortcut name\")\n    parser.add_argument(\"--paradigm\", default='annotation', type=str, required=True,\n                        help=\"\"\"The way to construct target sentence, selected from:[\n                            extraction:             (apple, positive); (banana, negative) //  None\n                            extraction-universal:  <pos> apple <pos> orange <neg> banana //   [none]\n                            ]\"\"\")\n    parser.add_argument(\"--do_train\", action='store_true', help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\", action='store_true', help=\"Whether to run eval on the dev/test set.\")\n    parser.add_argument(\"--name\", default='experinments', type=str, help=\"name of the exp\")\n    parser.add_argument(\"--commit\", default=None, type=str, help=\"commit id\")\n    parser.add_argument(\"--save_last_k\", default=5, type=int, help=\"save last k\")\n    parser.add_argument(\"--n_runs\", default=5, type=int, help=\"run with n seeds\")\n    parser.add_argument(\"--clear_model\", action='store_true', help=\"remove saved ckpts\")\n    parser.add_argument(\"--save_best\", action='store_true', help=\"save best model only\")\n    parser.add_argument(\"--nrows\", default=None, type=int, help=\"# of lines to be read\")\n\n    # basic setting\n    parser.add_argument(\"--train_by_pair\", action='store_true', help=\"train a model for each pair\")\n    parser.add_argument(\"--target_domain\", default=None, type=str)\n\n    # inference\n    parser.add_argument(\"--no_greedy\", action='store_true', help=\"only constrained decoding\")\n    parser.add_argument(\"--beam\", default=1, type=int)\n\n    # Other parameters\n    parser.add_argument(\"--max_seq_length\", default=128, type=int)\n    parser.add_argument(\"--n_gpu\", default=0)\n    parser.add_argument(\"--train_batch_size\", default=16, type=int,\n                        help=\"Batch size per GPU/CPU for training.\")\n    parser.add_argument(\"--eval_batch_size\", default=16, type=int,\n                        help=\"Batch size per GPU/CPU for evaluation.\")\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n    parser.add_argument(\"--learning_rate\", default=3e-4, type=float)\n    parser.add_argument(\"--num_train_epochs\", default=20, type=int,\n                        help=\"Total number of training epochs to perform.\")\n    parser.add_argument('--seed', type=int, default=42, help=\"random seed for initialization\")\n\n    # training details\n    parser.add_argument(\"--weight_decay\", default=0.0, type=float)\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float)\n    parser.add_argument(\"--warmup_steps\", default=0.0, type=float)\n\n    # template\n    parser.add_argument(\"--init_tag\", default=None, type=str, help=\"[english]\")\n\n    # data genenration\n    parser.add_argument(\"--data_gene\", action='store_true', help=\"enable data generation\")\n    parser.add_argument(\"--data_gene_epochs\", default=0, type=int, help=\"epoch of generation model training\")\n    parser.add_argument(\"--data_gene_extract\", action='store_true', help=\"enable text-to-label stage\")\n    parser.add_argument(\"--data_gene_extract_epochs\", default=0, type=int, help=\"epoch of extract model training\")\n    parser.add_argument(\"--data_gene_none_remove_ratio\", default=0, type=float, help=\"remove none input for gene model\")\n    parser.add_argument(\"--data_gene_none_word_num\", default=1, type=int, help=\"rand word added to generate diverse none examples\")\n    parser.add_argument(\"--data_gene_extract_none_remove_ratio\", default=0, type=float, help=\"remove none training sample for extract model training\")\n    parser.add_argument(\"--data_gene_same_model\", action='store_true', help=\"extract & gene model share the same model\")\n    parser.add_argument(\"--data_gene_wt_constrained\", action='store_true', help=\"turn off constrained decoding during gene generation\")\n    parser.add_argument(\"--use_same_model\", action='store_true', help=\"all stages use the same model\")\n    parser.add_argument(\"--model_filter\", action='store_true', help=\"use extract model inference for filtering\")\n    parser.add_argument(\"--model_filter_skip_none\", action='store_true', help=\"keep the sample if extract model output none\")\n\n    # decode\n    parser.add_argument(\"--data_gene_decode\", default=None, type=str, help=\"[greedy, top_p, beam]\")\n    parser.add_argument(\"--data_gene_top_p\", default=0.9, type=float)\n    parser.add_argument(\"--data_gene_num_beam\", default=1, type=int)\n    parser.add_argument(\"--data_gene_min_length\", default=0, type=int)\n    parser.add_argument(\"--extract_model\", default=None, type=str, help=\"path to extract model\")\n    parser.add_argument(\"--gene_model\", default=None, type=str, help=\"path to gene model\")\n\n    parser.add_argument(\"--runned_folder\", default=None, type=str, help=\"Load previous trained model for aux training\")\n    parser.add_argument(\"--data_gene_aug_num\", default=None, type=int, help=\"how many augmentation samples\")\n    parser.add_argument(\"--data_gene_aug_ratio\", default=None, type=float, help=\"how much ratio of augmentation samples\")\n\n    # pseudo labelling\n    parser.add_argument(\"--pseudo\", action='store_true', help=\"data generation\")\n    parser.add_argument(\"--pseudo_skip_none\", action='store_true', help=\"use non-none only\")\n\n    args = parser.parse_args()\n\n    # Set up output directory\n    output_dir = f\"../outputs/{args.task}/{args.dataset}\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create a timestamped directory\n    timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n    output_dir = os.path.join(output_dir, f\"{timestamp}-{args.name}\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    args.output_dir = output_dir\n\n    return args\n\n\ndef prepare_seeds(seed=42, n_runs=1):\n    # prepare n seeds\n    random.seed(seed)\n    seed_list = random.sample(range(100), n_runs-1)\n    seed_list += [seed]\n    seed_list = sorted(seed_list)\n    print(f\"Seed list: {seed_list}\")\n    return seed_list\n\n\ndef prepare_pairs(args):\n    # Define the mapping between tasks, datasets, and pairs\n    task_dataset_pairs = {\n        \"uabsa\": {\"cross_domain\": UABSA_TRANSFER_PAIRS},\n        \"ate\": {\"cross_domain\": UABSA_TRANSFER_PAIRS},\n        \"aste\": {\"cross_domain\": ASTE_TRANSFER_PAIRS},\n        \"aope\": {\"cross_domain\": AOPE_TRANSFER_PAIRS},\n    }\n\n    # Check if the task and dataset combination is valid and retrieve the corresponding pair\n    pair_dict = task_dataset_pairs.get(args.task, {}).get(args.dataset)\n\n    if pair_dict is None:\n        raise NotImplementedError(\"The task and/or dataset is not implemented.\")\n\n    return pair_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:22:01.251327Z","iopub.execute_input":"2023-11-02T15:22:01.251933Z","iopub.status.idle":"2023-11-02T15:22:01.284544Z","shell.execute_reply.started":"2023-11-02T15:22:01.251902Z","shell.execute_reply":"2023-11-02T15:22:01.283441Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!sed -i '1s/.*/from collections.abc import Mapping, Sequence/' /opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/apply_func.py","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:22:01.285885Z","iopub.execute_input":"2023-11-02T15:22:01.286184Z","iopub.status.idle":"2023-11-02T15:22:02.351128Z","shell.execute_reply.started":"2023-11-02T15:22:01.286159Z","shell.execute_reply":"2023-11-02T15:22:02.349811Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# main.py\nimport json\nimport os\nimport sys\n\nfrom pytorch_lightning import seed_everything\nfrom transformers import (AutoModelForSeq2SeqLM, T5Tokenizer)\n\ndef prepare_logger(args):\n    # Setup logging\n    logging.basicConfig(format='%(asctime)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    formatter = logFormatter = logging.Formatter(fmt='[%(asctime)s - %(name)s:%(lineno)d]: %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n    log_file = os.path.join(args.seed_dir, \"run.log\")\n    file_handler = logging.FileHandler(log_file, mode=\"w\", encoding='utf-8', delay=False)\n    console_handler = logging.StreamHandler(sys.stdout)\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    logger.handlers = [console_handler, file_handler]\n\n\ndef evaluate(args, tokenizer, dataset, model, paradigm, task, sents, split, is_constrained, eval_set_count_dict=None, silent=False):\n    \"\"\"\n    Compute scores given the predictions and gold labels\n    \"\"\"\n\n    decode_txt = \"constrained\" if is_constrained else \"greedy\"\n    logger.info(f\"Eval set count dict: {eval_set_count_dict}\")\n\n    inputs, outputs, targets = infer(\n        args, dataset, model, tokenizer, name=f\"{split}_{task}\",\n        is_constrained=is_constrained, constrained_vocab=prepare_constrained_tokens(tokenizer, task, paradigm),\n    )\n\n    start_idx = 0\n    score_dict = {\n        \"raw_scores\": {k: 0 for k in eval_set_count_dict.keys()},\n        \"fixed_scores\": {k: 0 for k in eval_set_count_dict.keys()},\n    }\n    pred_dict = {\n        \"labels\": {k: 0 for k in eval_set_count_dict.keys()},\n        \"preds\": {k: 0 for k in eval_set_count_dict.keys()},\n        \"preds_fixed\": {k: 0 for k in eval_set_count_dict.keys()},\n    }\n\n    verbose = True if split == \"dev\" else False\n    for l, num in eval_set_count_dict.items():\n        raw_score, fixed_score, label, pred, pred_fixed = compute_scores(\n                                                outputs[start_idx: start_idx+num],\n                                                targets[start_idx: start_idx+num],\n                                                sents[start_idx: start_idx+num],\n                                                paradigm, task, verbose)\n        start_idx += num\n\n        score_dict[\"raw_scores\"][l] = raw_score\n        score_dict[\"fixed_scores\"][l] = fixed_score\n        pred_dict[\"labels\"][l] = label\n        pred_dict[\"preds\"][l] = pred\n        pred_dict[\"preds_fixed\"][l] = pred_fixed\n\n    \"\"\"\n        score_dict = {\n            \"raw_scores\": {\n                \"en\": {\"precision\": 0, \"recall\": 0, \"f1\": 0},\n                \"all\": {\"precision\": 0, \"recall\": 0, \"f1\": 0},\n            }\n            \"fixed_score\": {\n            }\n        }\n    \"\"\"\n\n    if not silent:\n        for score_type in [\"raw_scores\", \"fixed_scores\"]:\n\n            logger.info('='*100)\n            logger.info(score_type)\n            logger.info('\\t'.join(list(score_dict[score_type].keys())))\n            f1_list = [i[\"f1\"] for i in list(score_dict[score_type].values())]\n            logger.info('\\t'.join([f\"{i*100:.2f}\" for i in f1_list]))\n\n    with open(os.path.join(args.score_dir, f\"{split}_{decode_txt}_errors.txt\"), \"w\", encoding='utf-8') as f:\n        counter = 0\n        for lang in pred_dict['labels'].keys():\n            for i in range(len(pred_dict['labels'][lang])):\n                label_i = pred_dict['labels'][lang][i]\n                pred_i = pred_dict['preds'][lang][i]\n                pred_fixed_i = pred_dict['preds_fixed'][lang][i]\n                test_i = ' '.join(sents[counter])\n                if label_i != pred_i:\n                    f.write(f\"{test_i} === {label_i} === {pred_i} === {pred_fixed_i} \\n\")\n                counter += 1\n\n    # return  {k: score_dict[\"fixed_scores\"][\"all w/t en\"][k] for k in [\"precision\", \"recall\", \"f1\"]}\n    return score_dict, pred_dict\n\n\ndef main(args):\n    prepare_logger(args)\n    start_info = \"=\"*30+f\"NEW EXP: {args.task.upper()} on {args.dataset} with seed {args.seed}\"+\"=\"*30\n    logger.info(\"#\"*len(start_info))\n    logger.info(start_info)\n    logger.info(\"#\"*len(start_info))\n\n    # Initialize args\n    args.ori_tok_len = T5_ORI_LEN\n    args.data_dir = os.path.join(args.seed_dir, \"data\")\n    args.inference_dir = os.path.join(args.seed_dir, \"inference\")\n    args.score_dir = os.path.join(args.seed_dir, \"score\")\n\n    # Create inference and score directories if they do not exist\n    for directory in [args.inference_dir, args.score_dir]:\n        os.makedirs(directory, exist_ok=True)\n\n    # Log and save args\n    logger.info(args)\n    json.dump(args.__dict__, open(args.seed_dir+\"/args.json\", 'w', encoding='UTF-8'), indent=2)\n\n    seed_everything(args.seed)\n\n    # prepare raw data, inlcudes [train.txt, test.txt, dev.txt, target-unlabel.txt] etc.\n    eval_count_dict, test_count_dict = prepare_raw_data(args)\n\n    tag_tokens = prepare_tag_tokens(args)\n\n    tokenizer = T5Tokenizer.from_pretrained(args.model_name_or_path, legacy=False)\n    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n\n    # training process\n    if args.do_train:\n        # add special tokens\n        special_tokens = tag_tokens\n        tokenizer.add_tokens(special_tokens)\n        model.resize_token_embeddings(len(tokenizer))\n        logger.info(f\"Tokens added into embedding: {special_tokens}\")\n        logger.info(f\"Tokenizer len: {len(tokenizer)}\")\n\n        if args.init_tag:\n            init_tag(args, tokenizer, model, tag_tokens)\n\n        # show one sample to check the sanity of the code and the expected output\n        logger.info(f\"Here is an example (from dev set) under `{args.paradigm}` paradigm:\")\n        dev_dataset = get_dataset(args, task=args.task, data_type=\"dev\", tokenizer=tokenizer)\n        for i in range(3):\n            logger.info(f'Input : {tokenizer.decode(dev_dataset[i][\"source_ids\"], skip_special_tokens=True)}')\n            logger.info(f'Output: {tokenizer.decode(dev_dataset[i][\"target_ids\"], skip_special_tokens=True)}')\n\n        model.to(args.device)\n\n        train_dataset = get_dataset(args, task=args.task, data_type=\"train\", tokenizer=tokenizer)\n\n        # aux training, inlcude [data_gene, pseudo]\n        aux_outputs = aux_training(args, tokenizer, model, train_dataset)\n        if args.data_gene or args.pseudo:\n            if args.data_gene:\n                train_dataset = aux_outputs[\"data_gene_dataset\"]\n            if args.pseudo:\n                train_dataset = aux_outputs[\"pseudo_dataset\"]\n            # reload model\n            if args.use_same_model:\n                logger.info(f\"Use the same model.\")\n            else:\n                model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n                model.to(args.device)\n                logger.info(f\"Model reloaded with {args.model_name_or_path}\")\n                model.resize_token_embeddings(len(tokenizer))\n                logger.info(f\"Tokens added into embedding: {special_tokens}\")\n\n        # official training\n        train(args, tokenizer, model, train_dataset, task=args.task, epochs=args.num_train_epochs, lr=args.learning_rate,\n        bs=args.train_batch_size, save_ckpt=True)\n\n    if args.do_eval:\n\n        logger.info(\"*\"*20+\" Conduct Evaluating\"+\"*\"*20)\n\n        all_checkpoints = []\n        # retrieve all the saved checkpoints for model selection\n        saved_model_dir = args.seed_dir\n        if args.do_train:\n            for f in os.listdir(saved_model_dir):\n                file_name = os.path.join(saved_model_dir, f)\n                if 'checkpoint' in file_name:\n                    all_checkpoints.append(file_name)\n        else:\n            all_checkpoints.append(args.model_name_or_path)\n            tokenizer = T5Tokenizer.from_pretrained(args.model_name_or_path, legacy=False)\n\n        # conduct some selection (or not)\n        logger.info(f\"We will perform validation on the following checkpoints: {all_checkpoints}\")\n\n        # load dev and test datasets\n        test_sents, _ = read_line_examples_from_file(f\"{args.data_dir}/test.txt\")\n        dev_sents, _ = read_line_examples_from_file(f\"{args.data_dir}/dev.txt\")\n\n        dev_dataset = get_dataset(args, task=args.task, data_type=\"dev\", tokenizer=tokenizer)\n        test_dataset = get_dataset(args, task=args.task, data_type=\"test\", tokenizer=tokenizer)\n\n        decode_list = [False, True]\n        if args.no_greedy:\n            decode_list = [True]\n        for is_constrained in decode_list:\n\n            decode_txt = \"constrained\" if is_constrained else \"greedy\"\n            logger.info(\"%\"*100)\n            logger.info(f\"Decode by {decode_txt}\")\n            best_f1, best_checkpoint, best_epoch = -999999.0, None, None\n            best_score_dict, best_pred_dict = None, None\n            all_epochs = []\n\n            score_dicts = { \"dev\": [], \"test\": []}\n\n            for checkpoint in all_checkpoints:\n                epoch = checkpoint.split('-')[-1][1:]\n                all_epochs.append(epoch)\n\n                # reload the model and conduct inference\n                logger.info(f\"Load the trained model from {checkpoint}...\")\n                model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n                model.to(args.device)\n\n                score_dict_dev, pred_dict_dev = evaluate(args, tokenizer, dev_dataset, model, args.paradigm, args.task, dev_sents, \"dev\",\n                                                        is_constrained=is_constrained, eval_set_count_dict=eval_count_dict, silent=True)\n                score_dict_test, pred_dict_test = evaluate(args, tokenizer, test_dataset, model, args.paradigm, args.task, test_sents, \"test\",\n                                                        is_constrained=is_constrained, eval_set_count_dict=test_count_dict, silent=True)\n\n                if args.dataset == \"cross_domain\":\n                    best_condition = score_dict_dev[\"raw_scores\"][args.source_domain]['f1']\n                if best_condition > best_f1:\n                    best_f1 = best_condition\n                    best_checkpoint = checkpoint\n                    best_epoch = epoch\n                    best_score_dict = score_dict_test\n                    best_pred_dict = pred_dict_test\n\n                score_dicts[\"dev\"].append(score_dict_dev)\n                score_dicts[\"test\"].append(score_dict_test)\n\n            json.dump(best_score_dict, open(f\"{args.score_dir}/test_{decode_txt}_score.json\", 'w', encoding='UTF-8'), indent=4)\n            json.dump(best_pred_dict, open(f\"{args.score_dir}/test_{decode_txt}_pred.json\", 'w', encoding='UTF-8'), indent=4)\n\n            # visualize evaluation results\n            for split_i in [\"dev\", \"test\"]:\n                separator = \"\\t\"\n                logger.info('='*100)\n                logger.info(f\"{split_i} result over from epochs {all_epochs}\")\n                for score_type in [\"raw_scores\", \"fixed_scores\"]:\n                    logger.info(score_type)\n                    langs = list(score_dicts[split_i][0][score_type].keys())\n                    logger.info(f\"lang{separator}\"+f\"{separator}\".join(langs))\n                    for i, epoch_i in enumerate(all_epochs):\n                        if epoch_i == best_epoch:\n                            print_line = f\"epoch-{epoch_i}[best]{separator}\"\n                        else:\n                            print_line = f\"epoch-{epoch_i}{separator}\"\n                        for lang in langs:\n                            for metric_i in [\"precision\", \"recall\", \"f1\"]:\n                                print_line += \"{:.2f}\".format(100*score_dicts[split_i][i][score_type][lang][metric_i])+\"/\"\n                            print_line += separator\n                        logger.info(print_line)\n\n            # print test results over last few steps\n            logger.info(f\"The best checkpoint is {best_checkpoint}\")\n\n        # only training's model needs to be deleted\n        if args.clear_model and args.do_train:\n            import shutil\n            for checkpoint in all_checkpoints:\n                if args.save_best and checkpoint == best_checkpoint:\n                        continue\n                else:\n                    shutil.rmtree(checkpoint)\n                    logger.info(f\"{checkpoint} is removed\")\n\n\ndef collate_seed_results(args, runed_dirs):\n    decode_txt_list = [\"constrained\"] if args.no_greedy else [\"greedy\", \"constrained\"]\n    for decode_txt in decode_txt_list:\n        logger.info(f\"Averaging {decode_txt}\")\n        if args.train_by_pair:\n            avg_n_seeds_by_pair(args.output_dir, runed_dirs, decode_txt, args.n_runs)\n        else:\n            raise NotImplementedError\n\n\ndef run_multiple_seeds(args, seed_list):\n\n    runed_dirs = []\n\n    for i in range(args.n_runs):\n\n        print(f\"Running with seed {seed_list[i]}\")\n\n        pair_dict = prepare_pairs(args)\n        for source in pair_dict:\n            if args.train_by_pair:\n                for target in pair_dict[source]:\n                    start_info = \"#\"*20+f\"Working on {source} --> {target}\"+\"$\"*20\n                    print(\"#\"*len(start_info))\n                    print(start_info)\n                    print(\"#\"*len(start_info))\n\n                    output_dir_i = f\"{args.output_dir}/seed-{seed_list[i]}/{source}-{target}\"\n                    os.makedirs(output_dir_i, exist_ok=True)\n                    runed_dirs.append(output_dir_i)\n                    args.seed_dir, args.seed = output_dir_i, seed_list[i]\n                    args.source_domain, args.target_domain = source, [target]\n                    main(args)\n            else:\n                raise NotImplementedError\n\n    return runed_dirs\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:22:02.352949Z","iopub.execute_input":"2023-11-02T15:22:02.353290Z","iopub.status.idle":"2023-11-02T15:22:03.069701Z","shell.execute_reply.started":"2023-11-02T15:22:02.353248Z","shell.execute_reply":"2023-11-02T15:22:03.068632Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class ABSAConfig():\n    task = 'aste'  # 'uabsa'\n    device = 'cuda'\n    dataset = 'cross_domain'\n    data_dir = '/kaggle/input/bgca-ko'\n    model_name_or_path = 'KETI-AIR/ke-t5-base-ko'  # 'mt5-base' # mt5-base가 다국어\n    paradigm = 'extraction-universal'  # 'annotation'\n    do_train = True  # action='store_true'\n    do_eval = True  # action='store_true'\n    name = 'experinments'\n    commit = None\n    save_last_k = 1  # 5\n    n_runs = 1  # 5\n    clear_model = True  # action='store_true'\n    save_best = True  # action='store_true'\n    nrows = None\n\n    # basic setting\n    train_by_pair = True  # action='store_true'\n    target_domain = None\n\n    # inference\n    no_greedy = False  # action='store_true'\n    beam = 1\n\n    # Other parameters\n    max_seq_length = 128\n    n_gpu = 0\n    train_batch_size = 4  # 16\n    eval_batch_size = 16\n    gradient_accumulation_steps = 2  # 1\n    learning_rate = 36e-5\n    num_train_epochs = 30  # 20\n    seed = 42\n\n    # training details\n    weight_decay = 0\n    adam_epsilon = 1e-8\n    warmup_steps = 0.0\n\n    # template\n    init_tag = 'english'  # None\n\n    # data genenration\n    data_gene = True  # action='store_true'\n    data_gene_epochs = 32 # 25  # 0\n    data_gene_extract = True  # action='store_true'\n    data_gene_extract_epochs = 32 # 25  # 0\n    data_gene_none_remove_ratio = 0\n    data_gene_none_word_num = 1\n    data_gene_extract_none_remove_ratio = 0\n    data_gene_same_model = False  # action='store_true'\n    data_gene_wt_constrained = True  # action='store_true'\n    use_same_model = True  # action='store_true'\n    model_filter = True  # action='store_true'\n    model_filter_skip_none = False  # action='store_true'\n\n    # decode\n    data_gene_decode = None\n    data_gene_top_p = 0.9\n    data_gene_num_beam = 1\n    data_gene_min_length = 0\n    extract_model = None\n    gene_model = None\n\n    runned_folder = None\n    data_gene_aug_num = None\n    data_gene_aug_ratio = None\n\n    # pseudo labelling\n    pseudo = False  # action='store_true'\n    pseudo_skip_none = False  # action='store_true'\n    def __init__(self):\n      # Set up output directory\n      output_dir = f\"{self.task}/{self.dataset}\"\n      os.makedirs(output_dir, exist_ok=True)\n\n      # Create a timestamped directory\n      timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n      output_dir = os.path.join(output_dir, f\"{timestamp}-{self.name}\")\n      os.makedirs(output_dir, exist_ok=True)\n\n      self.output_dir = output_dir","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:22:03.071275Z","iopub.execute_input":"2023-11-02T15:22:03.071665Z","iopub.status.idle":"2023-11-02T15:22:03.084563Z","shell.execute_reply.started":"2023-11-02T15:22:03.071637Z","shell.execute_reply":"2023-11-02T15:22:03.083333Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"absa_config = ABSAConfig()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:22:03.086339Z","iopub.execute_input":"2023-11-02T15:22:03.086730Z","iopub.status.idle":"2023-11-02T15:22:03.108446Z","shell.execute_reply.started":"2023-11-02T15:22:03.086694Z","shell.execute_reply":"2023-11-02T15:22:03.107284Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#args = init_args()\nseed_list = prepare_seeds(absa_config.seed, absa_config.n_runs)\nruned_dirs = run_multiple_seeds(absa_config, seed_list)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:22:03.109881Z","iopub.execute_input":"2023-11-02T15:22:03.110394Z","iopub.status.idle":"2023-11-02T16:58:29.876360Z","shell.execute_reply.started":"2023-11-02T15:22:03.110346Z","shell.execute_reply":"2023-11-02T16:58:29.875279Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Seed list: [42]\nRunning with seed 42\n#################################################################\n####################Working on blog --> merge$$$$$$$$$$$$$$$$$$$$\n#################################################################\n[11/02/2023 15:22:03 - BGCA:99]: ######################################################################################################\n[11/02/2023 15:22:03 - BGCA:100]: ==============================NEW EXP: ASTE on cross_domain with seed 42==============================\n[11/02/2023 15:22:03 - BGCA:101]: ######################################################################################################\n[11/02/2023 15:22:03 - BGCA:114]: <__main__.ABSAConfig object at 0x7ebb16798130>\n[11/02/2023 15:22:03 - BGCA:166]: train_paths: ['/kaggle/input/bgca-ko/aste/cross_domain/blog/train.txt']\n[11/02/2023 15:22:03 - BGCA:167]: dev_paths: ['/kaggle/input/bgca-ko/aste/cross_domain/blog/dev.txt']\n[11/02/2023 15:22:03 - BGCA:168]: test_paths: ['/kaggle/input/bgca-ko/aste/cross_domain/merge/test.txt']\n[11/02/2023 15:22:03 - BGCA:120]: train /kaggle/input/bgca-ko/aste/cross_domain/blog/train.txt: 552\n[11/02/2023 15:22:03 - BGCA:122]: train total: 552\n[11/02/2023 15:22:03 - BGCA:123]: train count dict: {None: 552}\n[11/02/2023 15:22:03 - BGCA:135]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/train.txt is written.\n[11/02/2023 15:22:03 - BGCA:120]: dev /kaggle/input/bgca-ko/aste/cross_domain/blog/dev.txt: 78\n[11/02/2023 15:22:03 - BGCA:122]: dev total: 78\n[11/02/2023 15:22:03 - BGCA:123]: dev count dict: {'blog': 78}\n[11/02/2023 15:22:03 - BGCA:135]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/dev.txt is written.\n[11/02/2023 15:22:03 - BGCA:120]: test /kaggle/input/bgca-ko/aste/cross_domain/merge/test.txt: 103\n[11/02/2023 15:22:03 - BGCA:122]: test total: 103\n[11/02/2023 15:22:03 - BGCA:123]: test count dict: {'merge': 103}\n[11/02/2023 15:22:03 - BGCA:135]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/test.txt is written.\n[11/02/2023 15:22:03 - BGCA:120]: target-unlabel /kaggle/input/bgca-ko/aste/cross_domain/merge/train.txt: 352\n[11/02/2023 15:22:03 - BGCA:122]: target-unlabel total: 352\n[11/02/2023 15:22:03 - BGCA:123]: target-unlabel count dict: {None: 352}\n[11/02/2023 15:22:03 - BGCA:135]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/target-unlabel.txt is written.\n[11/02/2023 15:22:03 - BGCA:157]: Cross domain unlabel train_paths: ['/kaggle/input/bgca-ko/aste/cross_domain/merge/train.txt']\n[11/02/2023 15:22:03 - BGCA:74]: Tag tokens: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.40M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a01b68401be043b4a8543104034dfad9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9372b5492fa44f7a7be22bcded2fdb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959d61836c124498b195ec6cdf4e8499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/602 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e6098dc04934b77b4257c421f812737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/944M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2d9ecf2398420f9376c5a14de6e3a8"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:22:46 - BGCA:133]: Tokens added into embedding: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n[11/02/2023 15:22:46 - BGCA:134]: Tokenizer len: 64105\n[11/02/2023 15:22:46 - BGCA:95]: <neg> is init by negative\n[11/02/2023 15:22:46 - BGCA:95]: <pos> is init by positive\n[11/02/2023 15:22:46 - BGCA:95]: <neu> is init by neutral\n[11/02/2023 15:22:46 - BGCA:95]: <opinion> is init by opinion\n[11/02/2023 15:22:46 - BGCA:140]: Here is an example (from dev set) under `extraction-universal` paradigm:\n[11/02/2023 15:22:46 - BGCA:87]: dev.txt\tTotal examples = 78 \n[11/02/2023 15:22:46 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/dev_aste_processed.txt is written.\n[11/02/2023 15:22:47 - BGCA:143]: Input : 이병헌과 정우성의 캐릭터는 그야말로 딱 중간 정도\n[11/02/2023 15:22:47 - BGCA:144]: Output: <neu> 이병헌과 정우성의 캐릭터 <opinion> 딱 중간 정도\n[11/02/2023 15:22:47 - BGCA:143]: Input : 플래시는 우리나라에서 90년대에 방영한 미드 때문에 원래부터 좋아했고 최근에 CW 미드로 다시 만들어진 플래시 역시 재미있게 봤기에 영화에서의 등장이 유난히 반가웠다\n[11/02/2023 15:22:47 - BGCA:144]: Output: <pos> 플래시 <opinion> 역시 재미있게\n[11/02/2023 15:22:47 - BGCA:143]: Input : 그것도 80년대 최고의 오락영화의 정통계보를 잇는 인디아나존스 가 재등장 한 것은 반가운 일이다\n[11/02/2023 15:22:47 - BGCA:144]: Output: <pos> 인디아나존스 <opinion> 반가운 일이다\n[11/02/2023 15:22:52 - BGCA:87]: train.txt\tTotal examples = 552 \n[11/02/2023 15:22:52 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/train_aste_processed.txt is written.\n[11/02/2023 15:22:53 - BGCA:87]: train.txt\tTotal examples = 552 \n[11/02/2023 15:22:53 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/train_extract_aste_processed.txt is written.\n[11/02/2023 15:22:53 - BGCA:29]: ######################################################################\n[11/02/2023 15:22:53 - BGCA:30]: #################### Conduct extract_aste Training####################\n[11/02/2023 15:22:53 - BGCA:31]: ######################################################################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6713886f7bf14d248af19c273d1be557"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:22:53 - BGCA:53]: Training examples out of 552:\n[11/02/2023 15:22:53 - BGCA:55]: Input : 조폭이면서 저렇게까지 말도 안되는 효자는 이세상에 없지만 영화로써의 재미는 충분하다\n[11/02/2023 15:22:54 - BGCA:56]: Output: <pos> 영화 <opinion> 재미는 충분하다\n[11/02/2023 15:22:54 - BGCA:55]: Input : 또 중간에 잠깐 지루하긴 했지만 스토리 라인이 탄탄했어요\n[11/02/2023 15:22:54 - BGCA:56]: Output: <pos> 스토리 라인 <opinion> 탄탄했어요\n[11/02/2023 15:22:54 - BGCA:55]: Input : 배우들의 배역 또한 절묘하여 좋다\n[11/02/2023 15:22:54 - BGCA:56]: Output: <pos> 배우들의 배역 <opinion> 절묘하여 좋다\n[11/02/2023 15:22:54 - BGCA:58]: Model emb weights of <pad> tensor([ 2.3125, -0.3555,  0.6797, -0.1768,  0.9062], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5802d45829f248b3b941a22ac552dfd3"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:23:24 - BGCA:96]: Epoch 0 Avg epoch train loss: 32.27726 lr: [0.00034875000000000005, 0.00034875000000000005]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c4e04af1cf440f1b8db841868c941fe"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:23:52 - BGCA:96]: Epoch 1 Avg epoch train loss: 15.74198 lr: [0.0003375, 0.0003375]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba325a51426f43b38c9453c49f679b86"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:24:20 - BGCA:96]: Epoch 2 Avg epoch train loss: 7.14569 lr: [0.00032625000000000004, 0.00032625000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb6b8f83e1e0491d91f821c092efa94c"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:24:48 - BGCA:96]: Epoch 3 Avg epoch train loss: 5.58351 lr: [0.000315, 0.000315]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ece9b07efa3403c8ada72575301a647"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:25:16 - BGCA:96]: Epoch 4 Avg epoch train loss: 4.79979 lr: [0.00030375000000000004, 0.00030375000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56e28ba090814ba396f14e0c1297d913"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:25:44 - BGCA:96]: Epoch 5 Avg epoch train loss: 4.07646 lr: [0.0002925, 0.0002925]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef0ef40f1de4d4aa85954323d64fa31"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:26:12 - BGCA:96]: Epoch 6 Avg epoch train loss: 3.39137 lr: [0.00028125000000000003, 0.00028125000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed231ef73834f74b68d68a02ce0ef6f"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:26:40 - BGCA:96]: Epoch 7 Avg epoch train loss: 2.84567 lr: [0.00027, 0.00027]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c418f41985413f9d4be71fba9b160e"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:27:08 - BGCA:96]: Epoch 8 Avg epoch train loss: 2.38722 lr: [0.00025875000000000003, 0.00025875000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85eb75dae5864773985573ceda77269a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:27:36 - BGCA:96]: Epoch 9 Avg epoch train loss: 2.00013 lr: [0.0002475, 0.0002475]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1c2613ca85746d890f995258fae1fff"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:28:04 - BGCA:96]: Epoch 10 Avg epoch train loss: 1.67322 lr: [0.00023625000000000002, 0.00023625000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"780b507932604c53a0d680dde1e3f758"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:28:32 - BGCA:96]: Epoch 11 Avg epoch train loss: 1.36395 lr: [0.00022500000000000002, 0.00022500000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ee6959032234be2afbf56462cedbfeb"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:29:00 - BGCA:96]: Epoch 12 Avg epoch train loss: 1.16704 lr: [0.00021375000000000002, 0.00021375000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9268e8dec034e439e12812b77ff165d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:29:28 - BGCA:96]: Epoch 13 Avg epoch train loss: 0.99035 lr: [0.00020250000000000002, 0.00020250000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7d6fdb1a224b11b15fa209f6871b31"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:29:56 - BGCA:96]: Epoch 14 Avg epoch train loss: 0.84298 lr: [0.00019125000000000001, 0.00019125000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1334876ec9964a7695489b57863a848a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:30:24 - BGCA:96]: Epoch 15 Avg epoch train loss: 0.70433 lr: [0.00018, 0.00018]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdcaaa723f884a3ea0ba3d1da38e9d34"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:30:52 - BGCA:96]: Epoch 16 Avg epoch train loss: 0.63339 lr: [0.00016875, 0.00016875]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e54fce24bc3f464797e135e0be9cf7c8"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:31:21 - BGCA:96]: Epoch 17 Avg epoch train loss: 0.55198 lr: [0.0001575, 0.0001575]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e83bdd478b4afe9c35941dd3ff56e3"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:31:49 - BGCA:96]: Epoch 18 Avg epoch train loss: 0.45654 lr: [0.00014625, 0.00014625]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e22b32913c29437cb85cbca2f40fa7b0"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:32:17 - BGCA:96]: Epoch 19 Avg epoch train loss: 0.41687 lr: [0.000135, 0.000135]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7625cf79417846ec821d6dae408ff35d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:32:45 - BGCA:96]: Epoch 20 Avg epoch train loss: 0.36808 lr: [0.00012375, 0.00012375]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b48673ba58954a22a0b31233c5cb7c25"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:33:13 - BGCA:96]: Epoch 21 Avg epoch train loss: 0.34630 lr: [0.00011250000000000001, 0.00011250000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e11d3490f1f43e1ab36ad53ee652f2a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:33:41 - BGCA:96]: Epoch 22 Avg epoch train loss: 0.31138 lr: [0.00010125000000000001, 0.00010125000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"290a2fb84c474bb58c09aa90b95de59b"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:34:09 - BGCA:96]: Epoch 23 Avg epoch train loss: 0.28730 lr: [9e-05, 9e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea343b8706b04418ad9adbb183bee7d3"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:34:37 - BGCA:96]: Epoch 24 Avg epoch train loss: 0.26522 lr: [7.875e-05, 7.875e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2751517326ac4c9cb97be35dd3c8f56b"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:35:05 - BGCA:96]: Epoch 25 Avg epoch train loss: 0.23216 lr: [6.75e-05, 6.75e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d10ead980a0040808f0424010ecf5901"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:35:33 - BGCA:96]: Epoch 26 Avg epoch train loss: 0.21782 lr: [5.6250000000000005e-05, 5.6250000000000005e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b07a254f91574b76b9840f3849647d20"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:36:01 - BGCA:96]: Epoch 27 Avg epoch train loss: 0.21060 lr: [4.5e-05, 4.5e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee0295cef4d146e9a1cfb93a23569a43"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:36:29 - BGCA:96]: Epoch 28 Avg epoch train loss: 0.20429 lr: [3.375e-05, 3.375e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5ac562dde6348f4899da6bacd6bf3aa"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:36:57 - BGCA:96]: Epoch 29 Avg epoch train loss: 0.20464 lr: [2.25e-05, 2.25e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"401ced1987c34408b3e0a410b0856f61"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:37:26 - BGCA:96]: Epoch 30 Avg epoch train loss: 0.19278 lr: [1.125e-05, 1.125e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b2771683d24307a7a57e47fae2dbd5"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:37:54 - BGCA:96]: Epoch 31 Avg epoch train loss: 0.17730 lr: [0.0, 0.0]\n[11/02/2023 15:37:55 - BGCA:104]: Save model checkpoint to aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/extract_aste-model\n[11/02/2023 15:37:55 - BGCA:106]: Finish training!\n[11/02/2023 15:37:55 - BGCA:87]: target-unlabel.txt\tTotal examples = 352 \n[11/02/2023 15:37:55 - BGCA:480]: Removed label for target-unlabel.\n[11/02/2023 15:37:55 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/target-unlabel_extract_aste_processed.txt is written.\n[11/02/2023 15:37:55 - BGCA:74]: Tag tokens: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n[11/02/2023 15:37:55 - BGCA:135]: Inferencing on target_extract_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"456b5cf18ed344008b34b51231232627"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:40:05 - BGCA:285]: Model keep the same.\n[11/02/2023 15:40:05 - BGCA:290]: Tokenizer len: 64105\n[11/02/2023 15:40:05 - BGCA:87]: train.txt\tTotal examples = 552 \n[11/02/2023 15:40:05 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/train_gene_aste_processed.txt is written.\n[11/02/2023 15:40:06 - BGCA:29]: ###################################################################\n[11/02/2023 15:40:06 - BGCA:30]: #################### Conduct gene_aste Training####################\n[11/02/2023 15:40:06 - BGCA:31]: ###################################################################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f6ca1b257784f45a094c550070030c1"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:40:06 - BGCA:53]: Training examples out of 552:\n[11/02/2023 15:40:06 - BGCA:55]: Input : <pos> 영화 <opinion> 재미는 충분하다\n[11/02/2023 15:40:06 - BGCA:56]: Output: 조폭이면서 저렇게까지 말도 안되는 효자는 이세상에 없지만 영화로써의 재미는 충분하다\n[11/02/2023 15:40:06 - BGCA:55]: Input : <pos> 스토리 라인 <opinion> 탄탄했어요\n[11/02/2023 15:40:06 - BGCA:56]: Output: 또 중간에 잠깐 지루하긴 했지만 스토리 라인이 탄탄했어요\n[11/02/2023 15:40:06 - BGCA:55]: Input : <pos> 배우들의 배역 <opinion> 절묘하여 좋다\n[11/02/2023 15:40:07 - BGCA:56]: Output: 배우들의 배역 또한 절묘하여 좋다\n[11/02/2023 15:40:07 - BGCA:58]: Model emb weights of <pad> tensor([ 2.3270, -0.3964,  0.6591, -0.2105,  0.9067], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"955b5f1bf1924dadb7ec9ad71a4325d7"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:40:35 - BGCA:96]: Epoch 0 Avg epoch train loss: 6.78619 lr: [0.00034875000000000005, 0.00034875000000000005]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eecdf985e1a74545a8c44c8281086a17"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:41:03 - BGCA:96]: Epoch 1 Avg epoch train loss: 5.46487 lr: [0.0003375, 0.0003375]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db05bb4fde44167a339d082bb208eee"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:41:31 - BGCA:96]: Epoch 2 Avg epoch train loss: 4.98515 lr: [0.00032625000000000004, 0.00032625000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad47546be5248589856653a90eb24a8"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:41:59 - BGCA:96]: Epoch 3 Avg epoch train loss: 4.53857 lr: [0.000315, 0.000315]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b20b613120456db7940de2b9b315c8"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:42:27 - BGCA:96]: Epoch 4 Avg epoch train loss: 4.25093 lr: [0.00030375000000000004, 0.00030375000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"155009fd36374c099ddcca41a29df41d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:42:55 - BGCA:96]: Epoch 5 Avg epoch train loss: 3.95173 lr: [0.0002925, 0.0002925]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87bae8cd01c64cbf8abb3d336ccf8a23"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:43:23 - BGCA:96]: Epoch 6 Avg epoch train loss: 3.68902 lr: [0.00028125000000000003, 0.00028125000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30027b836ca416194192c9d0e3ecb56"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:43:51 - BGCA:96]: Epoch 7 Avg epoch train loss: 3.41405 lr: [0.00027, 0.00027]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd25f27f50fd48969dbe608df6ce771d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:44:19 - BGCA:96]: Epoch 8 Avg epoch train loss: 3.22058 lr: [0.00025875000000000003, 0.00025875000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b7f00791a6c4aaa8fbfd6445bf1fa24"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:44:48 - BGCA:96]: Epoch 9 Avg epoch train loss: 2.93409 lr: [0.0002475, 0.0002475]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d05b598d71244048b2b8e9396e43daa8"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:45:16 - BGCA:96]: Epoch 10 Avg epoch train loss: 2.73035 lr: [0.00023625000000000002, 0.00023625000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efeaa8ef94d14b81a2be045161946058"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:45:44 - BGCA:96]: Epoch 11 Avg epoch train loss: 2.53647 lr: [0.00022500000000000002, 0.00022500000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baca2a274102470299ac396fe7c4594b"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:46:12 - BGCA:96]: Epoch 12 Avg epoch train loss: 2.32859 lr: [0.00021375000000000002, 0.00021375000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"973db5c93e5a4a8b98feda776fde2788"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:46:40 - BGCA:96]: Epoch 13 Avg epoch train loss: 2.21846 lr: [0.00020250000000000002, 0.00020250000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e0f598be8643cb9983c3de14f6671d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:47:08 - BGCA:96]: Epoch 14 Avg epoch train loss: 2.01143 lr: [0.00019125000000000001, 0.00019125000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dddd0cd2dd944773bbae5e780eac087a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:47:36 - BGCA:96]: Epoch 15 Avg epoch train loss: 1.90407 lr: [0.00018, 0.00018]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b08402e0eacd4380ad0b44a5c187c624"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:48:05 - BGCA:96]: Epoch 16 Avg epoch train loss: 1.77183 lr: [0.00016875, 0.00016875]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bae52921a2d469c913cd08e44ecc7fe"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:48:33 - BGCA:96]: Epoch 17 Avg epoch train loss: 1.61916 lr: [0.0001575, 0.0001575]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24979a3745d04dc0a216de5654c24a4d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:49:01 - BGCA:96]: Epoch 18 Avg epoch train loss: 1.55336 lr: [0.00014625, 0.00014625]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f16c7e5d1004ea19a0fe6d7e85a1c17"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:49:29 - BGCA:96]: Epoch 19 Avg epoch train loss: 1.44500 lr: [0.000135, 0.000135]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d104f7016a2940b2b1771ccf1ad16838"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:49:57 - BGCA:96]: Epoch 20 Avg epoch train loss: 1.37142 lr: [0.00012375, 0.00012375]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b266b5ecc6449629ca80651ce5470c0"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:50:25 - BGCA:96]: Epoch 21 Avg epoch train loss: 1.30662 lr: [0.00011250000000000001, 0.00011250000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3045a9cc944445be9281d430fd7f9d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:50:53 - BGCA:96]: Epoch 22 Avg epoch train loss: 1.24172 lr: [0.00010125000000000001, 0.00010125000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be36b910e4904a2d9da2fb30e21950ef"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:51:22 - BGCA:96]: Epoch 23 Avg epoch train loss: 1.16049 lr: [9e-05, 9e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce12d22091f144d5a9cefbf0a0ed0937"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:51:50 - BGCA:96]: Epoch 24 Avg epoch train loss: 1.11761 lr: [7.875e-05, 7.875e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac1a65ed23924d15a3470edc82de11a1"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:52:18 - BGCA:96]: Epoch 25 Avg epoch train loss: 1.06623 lr: [6.75e-05, 6.75e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e215bcd1a77348649e9d1f5fb3f84c41"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:52:46 - BGCA:96]: Epoch 26 Avg epoch train loss: 1.04841 lr: [5.6250000000000005e-05, 5.6250000000000005e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbe8fe08f59d41fc857a1b0f1bb81e95"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:53:14 - BGCA:96]: Epoch 27 Avg epoch train loss: 1.00205 lr: [4.5e-05, 4.5e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d88b045cbab546199c7fa3b8b8b30187"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:53:42 - BGCA:96]: Epoch 28 Avg epoch train loss: 0.95950 lr: [3.375e-05, 3.375e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2301c2743f9415e8f15e240e0bea072"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:54:10 - BGCA:96]: Epoch 29 Avg epoch train loss: 0.96708 lr: [2.25e-05, 2.25e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c805b54525749c3acaea26736914d62"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:54:39 - BGCA:96]: Epoch 30 Avg epoch train loss: 0.94725 lr: [1.125e-05, 1.125e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"595bc7860775489dae0deaddf159d474"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:55:07 - BGCA:96]: Epoch 31 Avg epoch train loss: 0.94559 lr: [0.0, 0.0]\n[11/02/2023 15:55:08 - BGCA:104]: Save model checkpoint to aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/gene_aste-model\n[11/02/2023 15:55:08 - BGCA:106]: Finish training!\n[11/02/2023 15:55:09 - BGCA:87]: target-unlabel.txt\tTotal examples = 352 \n[11/02/2023 15:55:09 - BGCA:74]: Tag tokens: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n[11/02/2023 15:55:09 - BGCA:365]: 4038 target domain words\n[11/02/2023 15:55:09 - BGCA:135]: Inferencing on target_gene ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"687f471c1e31446eafea2eefbd2eb32a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:57:23 - BGCA:32]: Output is invalid: <neg>\n[11/02/2023 15:57:23 - BGCA:32]: Output is invalid: <neg>\n[11/02/2023 15:57:23 - BGCA:32]: Output is invalid: <pos> 스타벅스 <opinion> 보이콧을에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에 <pos> 스타벅스 <opinion>\n[11/02/2023 15:57:23 - BGCA:32]: Output is invalid: <pos>\n[11/02/2023 15:57:23 - BGCA:32]: Output is invalid: <pos> 막걸리 <opinion> 더욱 좋았고 <pos> 목감도 <opinion> 최고고 <pos> 목감도 <opinion> 최고고 <pos> 목 목감도 <opinion> 최고고 <pos> 목 목감도 <opinion> 최고고 <pos> 목 목감도 <opinion> 최고고 <pos> 목 목감도 <opinion> 최고고 <pos> 목 목감도 <opinion> 최고고 <pos> 목 목감도 <opinion> 최고고 <pos> 목 목감도 <opinion> 최고고 <pos> 목 목감도 <opinion> 최고고 <pos> 목 목감도 <opinion> 너무 좋았고 <pos> 목 목감도 <opinion> 너무 좋았고 <pos> 목 목감도 <opinion> 너무 좋았고 <pos> 목도도 <opinion> 더욱 좋았고 <pos> 목감도도 최고고 <pos>\n[11/02/2023 15:57:23 - BGCA:32]: Output is invalid: <neg> ss <neg>\n[11/02/2023 15:57:23 - BGCA:32]: Output is invalid: <pos> 의료범죄 <opinion>\n[11/02/2023 15:57:23 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 15:57:23 - BGCA:33]: Filterd out 8 invalid samples\n[11/02/2023 15:57:27 - BGCA:381]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/extract_aste-model loaded.\n[11/02/2023 15:57:27 - BGCA:382]: Model emb weights of <pad> tensor([ 2.3270, -0.3964,  0.6591, -0.2105,  0.9067], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n[11/02/2023 15:57:27 - BGCA:74]: Tag tokens: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n[11/02/2023 15:57:27 - BGCA:135]: Inferencing on target_filter ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"080af7b1a6924e719c29b82a1fc0b018"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:59:58 - BGCA:411]: 335 augmentations out of 344 are removed by model.\n[11/02/2023 15:59:58 - BGCA:195]: Aug num after filtering: 9\n[11/02/2023 15:59:58 - BGCA:206]: Aug num final: 9\n[11/02/2023 15:59:58 - BGCA:117]: ['train_aste', 'target_gene_aug'] is merged.\n[11/02/2023 15:59:58 - BGCA:118]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/train_aste+target_gene_aug_merged_processed.txt is written.\n[11/02/2023 15:59:58 - BGCA:159]: Use the same model.\n[11/02/2023 15:59:58 - BGCA:29]: ##############################################################\n[11/02/2023 15:59:58 - BGCA:30]: #################### Conduct aste Training####################\n[11/02/2023 15:59:58 - BGCA:31]: ##############################################################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01510ea9210f48fca196b40ccf8d4d1b"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 15:59:58 - BGCA:53]: Training examples out of 561:\n[11/02/2023 15:59:58 - BGCA:55]: Input : 조폭이면서 저렇게까지 말도 안되는 효자는 이세상에 없지만 영화로써의 재미는 충분하다\n[11/02/2023 15:59:58 - BGCA:56]: Output: <pos> 영화 <opinion> 재미는 충분하다\n[11/02/2023 15:59:58 - BGCA:55]: Input : 또 중간에 잠깐 지루하긴 했지만 스토리 라인이 탄탄했어요\n[11/02/2023 15:59:58 - BGCA:56]: Output: <pos> 스토리 라인 <opinion> 탄탄했어요\n[11/02/2023 15:59:58 - BGCA:55]: Input : 배우들의 배역 또한 절묘하여 좋다\n[11/02/2023 15:59:58 - BGCA:56]: Output: <pos> 배우들의 배역 <opinion> 절묘하여 좋다\n[11/02/2023 15:59:58 - BGCA:58]: Model emb weights of <pad> tensor([ 2.3381, -0.4121,  0.6507, -0.2298,  0.9214], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e4f0821884e43bfb69631c0488d9d61"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:00:27 - BGCA:96]: Epoch 0 Avg epoch train loss: 0.86636 lr: [0.000348, 0.000348]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c9538d23ec486595c4a374f5730f8c"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:00:56 - BGCA:96]: Epoch 1 Avg epoch train loss: 0.29081 lr: [0.00033600000000000004, 0.00033600000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bf023c2e0b046b08dc93443a0e25720"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:01:24 - BGCA:96]: Epoch 2 Avg epoch train loss: 0.22892 lr: [0.000324, 0.000324]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"007078eecddb49df8ef09030f37b7ecd"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:01:53 - BGCA:96]: Epoch 3 Avg epoch train loss: 0.21631 lr: [0.00031200000000000005, 0.00031200000000000005]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"181b8caaf71e46e697940e2a074adb2e"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:02:22 - BGCA:96]: Epoch 4 Avg epoch train loss: 0.22746 lr: [0.00030000000000000003, 0.00030000000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012fcb41f9894eba8d6dea057cade40f"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:02:50 - BGCA:96]: Epoch 5 Avg epoch train loss: 0.15734 lr: [0.000288, 0.000288]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"550dfddc84874d76bf6ca474003f7761"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:03:19 - BGCA:96]: Epoch 6 Avg epoch train loss: 0.12498 lr: [0.00027600000000000004, 0.00027600000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00ac17668574322bc3da5d3cf78f853"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:03:48 - BGCA:96]: Epoch 7 Avg epoch train loss: 0.11887 lr: [0.000264, 0.000264]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c103f483df74daf95a41118ba1bdb86"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:04:16 - BGCA:96]: Epoch 8 Avg epoch train loss: 0.09569 lr: [0.000252, 0.000252]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2586ff118da742b9be5a54c6448f60d0"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:04:45 - BGCA:96]: Epoch 9 Avg epoch train loss: 0.09277 lr: [0.00024, 0.00024]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085f05ed54da4dd381fa878a84b16c5a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:05:14 - BGCA:96]: Epoch 10 Avg epoch train loss: 0.08454 lr: [0.000228, 0.000228]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf499828eed401293e498600fc9471b"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:05:42 - BGCA:96]: Epoch 11 Avg epoch train loss: 0.08288 lr: [0.00021600000000000002, 0.00021600000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7beebafe999d4609bc789d01160be4da"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:06:11 - BGCA:96]: Epoch 12 Avg epoch train loss: 0.07927 lr: [0.000204, 0.000204]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb8eb3c173c43c882df9a79baa8d8ac"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:06:40 - BGCA:96]: Epoch 13 Avg epoch train loss: 0.07477 lr: [0.000192, 0.000192]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeec4cc7f85f42f9b23e1ddae3809b68"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:07:08 - BGCA:96]: Epoch 14 Avg epoch train loss: 0.05158 lr: [0.00018, 0.00018]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ba62f387c840dbb3cea24944a0a7e4"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:07:37 - BGCA:96]: Epoch 15 Avg epoch train loss: 0.04808 lr: [0.00016800000000000002, 0.00016800000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a3100b9e2e477a89144b6827c6019d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:08:06 - BGCA:96]: Epoch 16 Avg epoch train loss: 0.05712 lr: [0.00015600000000000002, 0.00015600000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d858e4a8e6f4ef1ba9c602d4e40b58d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:08:34 - BGCA:96]: Epoch 17 Avg epoch train loss: 0.04559 lr: [0.000144, 0.000144]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f3001ebf9874dfb97b7e4bb74cc8d30"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:09:03 - BGCA:96]: Epoch 18 Avg epoch train loss: 0.04561 lr: [0.000132, 0.000132]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a0418d26e424ac5af62eb808a4f4305"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:09:32 - BGCA:96]: Epoch 19 Avg epoch train loss: 0.03416 lr: [0.00012, 0.00012]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"801b64b40286453ab1a222e0f87acec1"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:10:00 - BGCA:96]: Epoch 20 Avg epoch train loss: 0.03064 lr: [0.00010800000000000001, 0.00010800000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9fd4e8a8bd14cb59de002a3d2c92efa"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:10:29 - BGCA:96]: Epoch 21 Avg epoch train loss: 0.02648 lr: [9.6e-05, 9.6e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"776d01e8488d49c3b1799478a4dd7805"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:10:58 - BGCA:96]: Epoch 22 Avg epoch train loss: 0.02517 lr: [8.400000000000001e-05, 8.400000000000001e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73def9ff22db4133900ec5506f2ebf99"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:11:26 - BGCA:96]: Epoch 23 Avg epoch train loss: 0.02275 lr: [7.2e-05, 7.2e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494b2db19c1841aa96fecf5f1824219c"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:11:55 - BGCA:96]: Epoch 24 Avg epoch train loss: 0.02801 lr: [6e-05, 6e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bbab7877b714b5da4440f841afac896"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:12:23 - BGCA:96]: Epoch 25 Avg epoch train loss: 0.02447 lr: [4.8e-05, 4.8e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f0848fbdb3b40bf8e02a1bf1fe3e9fc"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:12:52 - BGCA:96]: Epoch 26 Avg epoch train loss: 0.01527 lr: [3.6e-05, 3.6e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c046fe8905a462e83eb9f845c93ccc0"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:13:21 - BGCA:96]: Epoch 27 Avg epoch train loss: 0.01313 lr: [2.4e-05, 2.4e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"537fde7f7ebb4d6988578e5f01be411e"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:13:49 - BGCA:96]: Epoch 28 Avg epoch train loss: 0.02186 lr: [1.2e-05, 1.2e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a2c2c435d18490fb9bddc011762f7ea"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:14:20 - BGCA:94]: Save model checkpoint to aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/checkpoint-e29\n[11/02/2023 16:14:20 - BGCA:96]: Epoch 29 Avg epoch train loss: 0.01522 lr: [0.0, 0.0]\n[11/02/2023 16:14:20 - BGCA:106]: Finish training!\n[11/02/2023 16:14:20 - BGCA:173]: ******************** Conduct Evaluating********************\n[11/02/2023 16:14:20 - BGCA:188]: We will perform validation on the following checkpoints: ['aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/checkpoint-e29']\n[11/02/2023 16:14:20 - BGCA:87]: test.txt\tTotal examples = 103 \n[11/02/2023 16:14:20 - BGCA:87]: dev.txt\tTotal examples = 78 \n[11/02/2023 16:14:20 - BGCA:87]: dev.txt\tTotal examples = 78 \n[11/02/2023 16:14:20 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/dev_aste_processed.txt is written.\n[11/02/2023 16:14:20 - BGCA:87]: test.txt\tTotal examples = 103 \n[11/02/2023 16:14:20 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/data/test_aste_processed.txt is written.\n[11/02/2023 16:14:20 - BGCA:203]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n[11/02/2023 16:14:20 - BGCA:204]: Decode by greedy\n[11/02/2023 16:14:20 - BGCA:216]: Load the trained model from aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/checkpoint-e29...\n[11/02/2023 16:14:23 - BGCA:27]: Eval set count dict: {'blog': 78}\n[11/02/2023 16:14:23 - BGCA:135]: Inferencing on dev_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e7b4e1c1344016bf811f92552fd0e8"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:14:38 - BGCA:360]: Gold: <neu> 이병헌과 정우성의 캐릭터 <opinion> 딱 중간 정도\n[11/02/2023 16:14:38 - BGCA:361]: Gold list: [('이병헌과 정우성의 캐릭터', '딱 중간 정도', 'neutral')]\n[11/02/2023 16:14:38 - BGCA:362]: Pred: <neu> 이병헌과 정우성의 캐릭터 <opinion> 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 <neu> 이병헌과 정우성의 캐릭터 <opinion> 딱 중간쯤\n[11/02/2023 16:14:38 - BGCA:363]: Pred list: [('이병헌과 정우성의 캐릭터', '딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱', 'neutral'), ('이병헌과 정우성의 캐릭터', '딱 중간쯤', 'neutral')]\n[11/02/2023 16:14:38 - BGCA:360]: Gold: <pos> 플래시 <opinion> 역시 재미있게\n[11/02/2023 16:14:38 - BGCA:361]: Gold list: [('플래시', '역시 재미있게', 'positive')]\n[11/02/2023 16:14:38 - BGCA:362]: Pred: <pos> 플래시 <opinion> 재미있게 본\n[11/02/2023 16:14:38 - BGCA:363]: Pred list: [('플래시', '재미있게 본', 'positive')]\n[11/02/2023 16:14:38 - BGCA:360]: Gold: <pos> 인디아나존스 <opinion> 반가운 일이다\n[11/02/2023 16:14:38 - BGCA:361]: Gold list: [('인디아나존스', '반가운 일이다', 'positive')]\n[11/02/2023 16:14:38 - BGCA:362]: Pred: <pos> 인디아나존스 <opinion> 반갑습니다\n[11/02/2023 16:14:38 - BGCA:363]: Pred list: [('인디아나존스', '반갑습니다', 'positive')]\n[11/02/2023 16:14:38 - BGCA:364]: Results of raw output\n[11/02/2023 16:14:38 - BGCA:365]: {'precision': 0.11494252873563218, 'recall': 0.11494252873563218, 'f1': 0.11494252873563218}\n[11/02/2023 16:14:38 - BGCA:366]: Results of fixed output\n[11/02/2023 16:14:38 - BGCA:367]: {'precision': 0.034482758620689655, 'recall': 0.034482758620689655, 'f1': 0.034482758620689655}\n[11/02/2023 16:14:38 - BGCA:27]: Eval set count dict: {'merge': 103}\n[11/02/2023 16:14:38 - BGCA:135]: Inferencing on test_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c7ad324a4144a7ca4f7a88b0ee54f79"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:15:14 - BGCA:243]: ====================================================================================================\n[11/02/2023 16:15:14 - BGCA:244]: dev result over from epochs ['29']\n[11/02/2023 16:15:14 - BGCA:246]: raw_scores\n[11/02/2023 16:15:14 - BGCA:248]: lang\tblog\n[11/02/2023 16:15:14 - BGCA:258]: epoch-29[best]\t11.49/11.49/11.49/\t\n[11/02/2023 16:15:14 - BGCA:246]: fixed_scores\n[11/02/2023 16:15:14 - BGCA:248]: lang\tblog\n[11/02/2023 16:15:14 - BGCA:258]: epoch-29[best]\t3.45/3.45/3.45/\t\n[11/02/2023 16:15:14 - BGCA:243]: ====================================================================================================\n[11/02/2023 16:15:14 - BGCA:244]: test result over from epochs ['29']\n[11/02/2023 16:15:14 - BGCA:246]: raw_scores\n[11/02/2023 16:15:14 - BGCA:248]: lang\tmerge\n[11/02/2023 16:15:14 - BGCA:258]: epoch-29[best]\t1.01/0.78/0.88/\t\n[11/02/2023 16:15:14 - BGCA:246]: fixed_scores\n[11/02/2023 16:15:14 - BGCA:248]: lang\tmerge\n[11/02/2023 16:15:14 - BGCA:258]: epoch-29[best]\t0.00/0.00/0.00/\t\n[11/02/2023 16:15:14 - BGCA:261]: The best checkpoint is aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/checkpoint-e29\n[11/02/2023 16:15:14 - BGCA:203]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n[11/02/2023 16:15:14 - BGCA:204]: Decode by constrained\n[11/02/2023 16:15:14 - BGCA:216]: Load the trained model from aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/checkpoint-e29...\n[11/02/2023 16:15:18 - BGCA:27]: Eval set count dict: {'blog': 78}\n[11/02/2023 16:15:18 - BGCA:135]: Inferencing on dev_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f3f87a07b634a0cb72e7b5a358693b7"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:15:40 - BGCA:360]: Gold: <neu> 이병헌과 정우성의 캐릭터 <opinion> 딱 중간 정도\n[11/02/2023 16:15:40 - BGCA:361]: Gold list: [('이병헌과 정우성의 캐릭터', '딱 중간 정도', 'neutral')]\n[11/02/2023 16:15:40 - BGCA:362]: Pred: <neu> 이병헌과 정우성의 캐릭터 <opinion> 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 <neu> 이병헌과 정우성의 캐릭터 <opinion> 딱 중간 정도\n[11/02/2023 16:15:40 - BGCA:363]: Pred list: [('이병헌과 정우성의 캐릭터', '딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱 딱', 'neutral'), ('이병헌과 정우성의 캐릭터', '딱 중간 정도', 'neutral')]\n[11/02/2023 16:15:40 - BGCA:360]: Gold: <pos> 플래시 <opinion> 역시 재미있게\n[11/02/2023 16:15:40 - BGCA:361]: Gold list: [('플래시', '역시 재미있게', 'positive')]\n[11/02/2023 16:15:40 - BGCA:362]: Pred: <pos> 플래시 <opinion> 재미있게 봤기에\n[11/02/2023 16:15:40 - BGCA:363]: Pred list: [('플래시', '재미있게 봤기에', 'positive')]\n[11/02/2023 16:15:40 - BGCA:360]: Gold: <pos> 인디아나존스 <opinion> 반가운 일이다\n[11/02/2023 16:15:40 - BGCA:361]: Gold list: [('인디아나존스', '반가운 일이다', 'positive')]\n[11/02/2023 16:15:40 - BGCA:362]: Pred: <pos> 인디아나존스 <opinion> 반가운\n[11/02/2023 16:15:40 - BGCA:363]: Pred list: [('인디아나존스', '반가운', 'positive')]\n[11/02/2023 16:15:40 - BGCA:364]: Results of raw output\n[11/02/2023 16:15:40 - BGCA:365]: {'precision': 0.3068181818181818, 'recall': 0.3103448275862069, 'f1': 0.3085714285714286}\n[11/02/2023 16:15:40 - BGCA:366]: Results of fixed output\n[11/02/2023 16:15:40 - BGCA:367]: {'precision': 0.045454545454545456, 'recall': 0.04597701149425287, 'f1': 0.04571428571428572}\n[11/02/2023 16:15:40 - BGCA:27]: Eval set count dict: {'merge': 103}\n[11/02/2023 16:15:40 - BGCA:135]: Inferencing on test_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"341431ca370a408093f7722fe958959d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:16:18 - BGCA:243]: ====================================================================================================\n[11/02/2023 16:16:18 - BGCA:244]: dev result over from epochs ['29']\n[11/02/2023 16:16:18 - BGCA:246]: raw_scores\n[11/02/2023 16:16:18 - BGCA:248]: lang\tblog\n[11/02/2023 16:16:18 - BGCA:258]: epoch-29[best]\t30.68/31.03/30.86/\t\n[11/02/2023 16:16:18 - BGCA:246]: fixed_scores\n[11/02/2023 16:16:18 - BGCA:248]: lang\tblog\n[11/02/2023 16:16:18 - BGCA:258]: epoch-29[best]\t4.55/4.60/4.57/\t\n[11/02/2023 16:16:18 - BGCA:243]: ====================================================================================================\n[11/02/2023 16:16:18 - BGCA:244]: test result over from epochs ['29']\n[11/02/2023 16:16:18 - BGCA:246]: raw_scores\n[11/02/2023 16:16:18 - BGCA:248]: lang\tmerge\n[11/02/2023 16:16:18 - BGCA:258]: epoch-29[best]\t5.68/3.91/4.63/\t\n[11/02/2023 16:16:18 - BGCA:246]: fixed_scores\n[11/02/2023 16:16:18 - BGCA:248]: lang\tmerge\n[11/02/2023 16:16:18 - BGCA:258]: epoch-29[best]\t0.00/0.00/0.00/\t\n[11/02/2023 16:16:18 - BGCA:261]: The best checkpoint is aste/cross_domain/1102_1522-experinments/seed-42/blog-merge/checkpoint-e29\n#################################################################\n####################Working on merge --> blog$$$$$$$$$$$$$$$$$$$$\n#################################################################\n[11/02/2023 16:16:18 - BGCA:99]: ######################################################################################################\n[11/02/2023 16:16:18 - BGCA:100]: ==============================NEW EXP: ASTE on cross_domain with seed 42==============================\n[11/02/2023 16:16:18 - BGCA:101]: ######################################################################################################\n[11/02/2023 16:16:18 - BGCA:114]: <__main__.ABSAConfig object at 0x7ebb16798130>\n[11/02/2023 16:16:18 - BGCA:166]: train_paths: ['/kaggle/input/bgca-ko/aste/cross_domain/merge/train.txt']\n[11/02/2023 16:16:18 - BGCA:167]: dev_paths: ['/kaggle/input/bgca-ko/aste/cross_domain/merge/dev.txt']\n[11/02/2023 16:16:18 - BGCA:168]: test_paths: ['/kaggle/input/bgca-ko/aste/cross_domain/blog/test.txt']\n[11/02/2023 16:16:18 - BGCA:120]: train /kaggle/input/bgca-ko/aste/cross_domain/merge/train.txt: 352\n[11/02/2023 16:16:18 - BGCA:122]: train total: 352\n[11/02/2023 16:16:18 - BGCA:123]: train count dict: {None: 352}\n[11/02/2023 16:16:18 - BGCA:135]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/train.txt is written.\n[11/02/2023 16:16:18 - BGCA:120]: dev /kaggle/input/bgca-ko/aste/cross_domain/merge/dev.txt: 50\n[11/02/2023 16:16:18 - BGCA:122]: dev total: 50\n[11/02/2023 16:16:18 - BGCA:123]: dev count dict: {'merge': 50}\n[11/02/2023 16:16:18 - BGCA:135]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/dev.txt is written.\n[11/02/2023 16:16:18 - BGCA:120]: test /kaggle/input/bgca-ko/aste/cross_domain/blog/test.txt: 159\n[11/02/2023 16:16:18 - BGCA:122]: test total: 159\n[11/02/2023 16:16:18 - BGCA:123]: test count dict: {'blog': 159}\n[11/02/2023 16:16:18 - BGCA:135]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/test.txt is written.\n[11/02/2023 16:16:18 - BGCA:120]: target-unlabel /kaggle/input/bgca-ko/aste/cross_domain/blog/train.txt: 552\n[11/02/2023 16:16:18 - BGCA:122]: target-unlabel total: 552\n[11/02/2023 16:16:18 - BGCA:123]: target-unlabel count dict: {None: 552}\n[11/02/2023 16:16:18 - BGCA:135]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/target-unlabel.txt is written.\n[11/02/2023 16:16:18 - BGCA:157]: Cross domain unlabel train_paths: ['/kaggle/input/bgca-ko/aste/cross_domain/blog/train.txt']\n[11/02/2023 16:16:18 - BGCA:74]: Tag tokens: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n[11/02/2023 16:16:23 - BGCA:133]: Tokens added into embedding: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n[11/02/2023 16:16:23 - BGCA:134]: Tokenizer len: 64105\n[11/02/2023 16:16:23 - BGCA:95]: <neg> is init by negative\n[11/02/2023 16:16:23 - BGCA:95]: <pos> is init by positive\n[11/02/2023 16:16:23 - BGCA:95]: <neu> is init by neutral\n[11/02/2023 16:16:23 - BGCA:95]: <opinion> is init by opinion\n[11/02/2023 16:16:23 - BGCA:140]: Here is an example (from dev set) under `extraction-universal` paradigm:\n[11/02/2023 16:16:23 - BGCA:87]: dev.txt\tTotal examples = 50 \n[11/02/2023 16:16:23 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/dev_aste_processed.txt is written.\n[11/02/2023 16:16:23 - BGCA:143]: Input : 하지만 최근 재판에서 김용호에게 이같은 허위사실을 제공한 사람이 박수홍의 형수라는 사실이 밝혀졌고 이에 박수홍 측은 형수에게 법적 대응을 예고한 바 있다\n[11/02/2023 16:16:23 - BGCA:144]: Output: <neg> 박수홍의 형수 <opinion> 허위사실을 제공한\n[11/02/2023 16:16:23 - BGCA:143]: Input : 모든 영역에서 선택과목을 없애는 내용의 수능 개편안에 대해서도 이들 단체는 국어 수학의 수능 영향력을 강화해 학생들의 수능 대비 사교육 의존도를 심화시킬 것이라고 주장했다\n[11/02/2023 16:16:23 - BGCA:144]: Output: <neg> 수능 개편안 <opinion> 사교육 의존도를 심화시킬\n[11/02/2023 16:16:23 - BGCA:143]: Input : 하마스 땅굴에는 여러 분기점과 거리 이름, 숫자가 붙어 있고, 전기나 석유, 물 공급 없이도 장기 체류할 수 있는 인프라까지 모두 갖추고 있다\n[11/02/2023 16:16:23 - BGCA:144]: Output: <pos> 땅굴 <opinion> 모두 갖추고 있다\n[11/02/2023 16:16:24 - BGCA:87]: train.txt\tTotal examples = 352 \n[11/02/2023 16:16:24 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/train_aste_processed.txt is written.\n[11/02/2023 16:16:24 - BGCA:87]: train.txt\tTotal examples = 352 \n[11/02/2023 16:16:24 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/train_extract_aste_processed.txt is written.\n[11/02/2023 16:16:24 - BGCA:29]: ######################################################################\n[11/02/2023 16:16:24 - BGCA:30]: #################### Conduct extract_aste Training####################\n[11/02/2023 16:16:24 - BGCA:31]: ######################################################################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8caa6b3c1d8d4d9a859a4149e297665f"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:16:24 - BGCA:53]: Training examples out of 352:\n[11/02/2023 16:16:24 - BGCA:55]: Input : 상습 마약 투약 혐의를 받는 배우 유아인 씨가 재판에 넘겨졌습니다\n[11/02/2023 16:16:24 - BGCA:56]: Output: <neg> 배우 유아인 <opinion> 혐의를 받는\n[11/02/2023 16:16:24 - BGCA:55]: Input : 평소 구하기 힘든 주류 상품들을 합리적인 가격에 구매하고자 하는 고객들의 니즈가 지속되고 있기 때문이다\n[11/02/2023 16:16:24 - BGCA:56]: Output: <pos> 구하기 힘든 주류 상품 <opinion> 합리적인\n[11/02/2023 16:16:24 - BGCA:55]: Input : 상대평가를 전 과목으로 확대하는 내신 개편안은 학생들의 입시 부담을 폭증시킬 것이라고 우려했다\n[11/02/2023 16:16:25 - BGCA:56]: Output: <neg> 내신 개편안 <opinion> 입시 부담을 폭증시킬\n[11/02/2023 16:16:25 - BGCA:58]: Model emb weights of <pad> tensor([ 2.3125, -0.3555,  0.6797, -0.1768,  0.9062], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"340d30557db84d30b02d5cd4b40e90d1"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:16:43 - BGCA:96]: Epoch 0 Avg epoch train loss: 35.77765 lr: [0.00034875000000000005, 0.00034875000000000005]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64459bbbb59f4058bce3749ffc24ec5d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:17:01 - BGCA:96]: Epoch 1 Avg epoch train loss: 20.96445 lr: [0.0003375, 0.0003375]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2bfee0e652c456aafad8c1a51b4bfd5"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:17:19 - BGCA:96]: Epoch 2 Avg epoch train loss: 15.66231 lr: [0.00032625000000000004, 0.00032625000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe16aec8dd6444e2a1298a53767cffe1"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:17:37 - BGCA:96]: Epoch 3 Avg epoch train loss: 10.22022 lr: [0.000315, 0.000315]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5f9cb1966304f7793dc94f43e251bd9"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:17:56 - BGCA:96]: Epoch 4 Avg epoch train loss: 6.92433 lr: [0.00030375000000000004, 0.00030375000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a07ac32fa84acb8e08d900767d9223"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:18:14 - BGCA:96]: Epoch 5 Avg epoch train loss: 6.18804 lr: [0.0002925, 0.0002925]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02368102c56543d4b2e93cead666eb23"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:18:32 - BGCA:96]: Epoch 6 Avg epoch train loss: 5.59028 lr: [0.00028125000000000003, 0.00028125000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c241eb4c962447a183611193ca670a1d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:18:50 - BGCA:96]: Epoch 7 Avg epoch train loss: 4.95275 lr: [0.00027, 0.00027]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b34962aa16d845c6bde893007eb1f65c"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:19:08 - BGCA:96]: Epoch 8 Avg epoch train loss: 4.43538 lr: [0.00025875000000000003, 0.00025875000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51bf85d87e674c3d9dd5d73eaf87878b"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:19:26 - BGCA:96]: Epoch 9 Avg epoch train loss: 3.91792 lr: [0.0002475, 0.0002475]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a016b30483e54c24a9077b837927c03c"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:19:44 - BGCA:96]: Epoch 10 Avg epoch train loss: 3.40156 lr: [0.00023625000000000002, 0.00023625000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f8d205cb80d400aae7ebf22962e449f"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:20:02 - BGCA:96]: Epoch 11 Avg epoch train loss: 3.05453 lr: [0.00022500000000000002, 0.00022500000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2964d291a2a54d8a8263fa6aa452d4c8"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:20:21 - BGCA:96]: Epoch 12 Avg epoch train loss: 2.70881 lr: [0.00021375000000000002, 0.00021375000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf886acca4554ac4aa9390550274a8e8"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:20:39 - BGCA:96]: Epoch 13 Avg epoch train loss: 2.32644 lr: [0.00020250000000000002, 0.00020250000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"247469f21adf415baa80592645a08e33"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:20:57 - BGCA:96]: Epoch 14 Avg epoch train loss: 2.08496 lr: [0.00019125000000000001, 0.00019125000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f0d04d411c4223995dff5a66014f27"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:21:15 - BGCA:96]: Epoch 15 Avg epoch train loss: 1.81697 lr: [0.00018, 0.00018]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912f1c3b9f674b43bcc560ec9fd688ac"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:21:33 - BGCA:96]: Epoch 16 Avg epoch train loss: 1.63200 lr: [0.00016875, 0.00016875]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d449f03b3b53484ebdbf0eb9a89a9350"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:21:51 - BGCA:96]: Epoch 17 Avg epoch train loss: 1.45971 lr: [0.0001575, 0.0001575]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93b59cd4983d4f56ac28d74761cc606b"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:22:09 - BGCA:96]: Epoch 18 Avg epoch train loss: 1.33347 lr: [0.00014625, 0.00014625]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522a4c18ef4d4937bcdf419e3e21d33c"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:22:28 - BGCA:96]: Epoch 19 Avg epoch train loss: 1.18977 lr: [0.000135, 0.000135]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe5375e5f2845f4ae9ff4ad215d9439"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:22:46 - BGCA:96]: Epoch 20 Avg epoch train loss: 1.06018 lr: [0.00012375, 0.00012375]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bfadfc5c44e4a93a76ef1aff82d10c7"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:23:04 - BGCA:96]: Epoch 21 Avg epoch train loss: 1.01450 lr: [0.00011250000000000001, 0.00011250000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b16b6e58ba747c492d060622cae7f5d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:23:22 - BGCA:96]: Epoch 22 Avg epoch train loss: 0.87144 lr: [0.00010125000000000001, 0.00010125000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4009857ac824b9590c16f156f1fc1bf"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:23:40 - BGCA:96]: Epoch 23 Avg epoch train loss: 0.80767 lr: [9e-05, 9e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa6640fbd6314c2cb1977c9c807800d6"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:23:58 - BGCA:96]: Epoch 24 Avg epoch train loss: 0.77756 lr: [7.875e-05, 7.875e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9b855cd7f00445d86d3c22d98e8c6da"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:24:16 - BGCA:96]: Epoch 25 Avg epoch train loss: 0.75120 lr: [6.75e-05, 6.75e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eed091851bd4163a22c42636e0d6669"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:24:35 - BGCA:96]: Epoch 26 Avg epoch train loss: 0.67764 lr: [5.6250000000000005e-05, 5.6250000000000005e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f81b068d24949349a8f75d6c8127b65"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:24:53 - BGCA:96]: Epoch 27 Avg epoch train loss: 0.61152 lr: [4.5e-05, 4.5e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1e4d37158f848c0bb4e7a8126d54743"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:25:11 - BGCA:96]: Epoch 28 Avg epoch train loss: 0.61741 lr: [3.375e-05, 3.375e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e45a8ccf42514450b044a21b7ff4c5d4"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:25:29 - BGCA:96]: Epoch 29 Avg epoch train loss: 0.60856 lr: [2.25e-05, 2.25e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa32340e7954ddcad33f6f08608e61e"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:25:47 - BGCA:96]: Epoch 30 Avg epoch train loss: 0.58635 lr: [1.125e-05, 1.125e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c03a478af9e8497eb0520007677a086a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:26:05 - BGCA:96]: Epoch 31 Avg epoch train loss: 0.58980 lr: [0.0, 0.0]\n[11/02/2023 16:26:07 - BGCA:104]: Save model checkpoint to aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/extract_aste-model\n[11/02/2023 16:26:07 - BGCA:106]: Finish training!\n[11/02/2023 16:26:07 - BGCA:87]: target-unlabel.txt\tTotal examples = 552 \n[11/02/2023 16:26:07 - BGCA:480]: Removed label for target-unlabel.\n[11/02/2023 16:26:07 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/target-unlabel_extract_aste_processed.txt is written.\n[11/02/2023 16:26:07 - BGCA:74]: Tag tokens: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n[11/02/2023 16:26:07 - BGCA:135]: Inferencing on target_extract_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fbc2e3aac494d96bddc5c7a3e83da80"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:29:32 - BGCA:285]: Model keep the same.\n[11/02/2023 16:29:32 - BGCA:290]: Tokenizer len: 64105\n[11/02/2023 16:29:32 - BGCA:87]: train.txt\tTotal examples = 352 \n[11/02/2023 16:29:32 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/train_gene_aste_processed.txt is written.\n[11/02/2023 16:29:33 - BGCA:29]: ###################################################################\n[11/02/2023 16:29:33 - BGCA:30]: #################### Conduct gene_aste Training####################\n[11/02/2023 16:29:33 - BGCA:31]: ###################################################################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e527ff8facc9469bbb7fb8d9ec822313"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:29:33 - BGCA:53]: Training examples out of 352:\n[11/02/2023 16:29:33 - BGCA:55]: Input : <neg> 배우 유아인 <opinion> 혐의를 받는\n[11/02/2023 16:29:33 - BGCA:56]: Output: 상습 마약 투약 혐의를 받는 배우 유아인 씨가 재판에 넘겨졌습니다\n[11/02/2023 16:29:33 - BGCA:55]: Input : <pos> 구하기 힘든 주류 상품 <opinion> 합리적인\n[11/02/2023 16:29:33 - BGCA:56]: Output: 평소 구하기 힘든 주류 상품들을 합리적인 가격에 구매하고자 하는 고객들의 니즈가 지속되고 있기 때문이다\n[11/02/2023 16:29:33 - BGCA:55]: Input : <neg> 내신 개편안 <opinion> 입시 부담을 폭증시킬\n[11/02/2023 16:29:33 - BGCA:56]: Output: 상대평가를 전 과목으로 확대하는 내신 개편안은 학생들의 입시 부담을 폭증시킬 것이라고 우려했다\n[11/02/2023 16:29:33 - BGCA:58]: Model emb weights of <pad> tensor([ 2.3119, -0.3872,  0.6557, -0.2127,  0.9099], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0f2d18a3b1d4d598c0874ea2c5822f3"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:29:51 - BGCA:96]: Epoch 0 Avg epoch train loss: 7.64392 lr: [0.00034875000000000005, 0.00034875000000000005]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b592113dea04311b6cfd929b0af5ace"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:30:10 - BGCA:96]: Epoch 1 Avg epoch train loss: 6.29245 lr: [0.0003375, 0.0003375]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3cd17500086406eadcaca180a15b110"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:30:28 - BGCA:96]: Epoch 2 Avg epoch train loss: 5.69170 lr: [0.00032625000000000004, 0.00032625000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7976003c1c3147f19f2717098031c1d9"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:30:46 - BGCA:96]: Epoch 3 Avg epoch train loss: 5.29442 lr: [0.000315, 0.000315]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cec66c44ebb4e4d863867c741d7ecd8"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:31:05 - BGCA:96]: Epoch 4 Avg epoch train loss: 4.90553 lr: [0.00030375000000000004, 0.00030375000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6a316aee2e4d6caeaf0ecdef5edbdb"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:31:23 - BGCA:96]: Epoch 5 Avg epoch train loss: 4.59268 lr: [0.0002925, 0.0002925]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c58308cc8434dc7b614a86dba3fa1bd"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:31:41 - BGCA:96]: Epoch 6 Avg epoch train loss: 4.26008 lr: [0.00028125000000000003, 0.00028125000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3842992c92dc4219947e8ab100770b42"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:31:59 - BGCA:96]: Epoch 7 Avg epoch train loss: 4.00164 lr: [0.00027, 0.00027]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d4cf43f4b14c99a840f3d895d05858"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:32:18 - BGCA:96]: Epoch 8 Avg epoch train loss: 3.73424 lr: [0.00025875000000000003, 0.00025875000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad64ef1ced9f4d02bae1419f77bc91f6"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:32:36 - BGCA:96]: Epoch 9 Avg epoch train loss: 3.49285 lr: [0.0002475, 0.0002475]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77df547ae9148308ab917366d8deda1"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:32:54 - BGCA:96]: Epoch 10 Avg epoch train loss: 3.27246 lr: [0.00023625000000000002, 0.00023625000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59e9abf10b664646932ca203373db3fe"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:33:12 - BGCA:96]: Epoch 11 Avg epoch train loss: 3.06773 lr: [0.00022500000000000002, 0.00022500000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df90ae4d0a394fb5a9842bce5a924f6e"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:33:31 - BGCA:96]: Epoch 12 Avg epoch train loss: 2.90947 lr: [0.00021375000000000002, 0.00021375000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ef7a7d257e4ad680c63155f37ab007"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:33:49 - BGCA:96]: Epoch 13 Avg epoch train loss: 2.74685 lr: [0.00020250000000000002, 0.00020250000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ede2d8ded94501a0cf9f96fbeb1cc4"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:34:07 - BGCA:96]: Epoch 14 Avg epoch train loss: 2.53825 lr: [0.00019125000000000001, 0.00019125000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5bdabec5759415eaffd62166e7c0273"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:34:25 - BGCA:96]: Epoch 15 Avg epoch train loss: 2.35459 lr: [0.00018, 0.00018]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed0b1e7382c4affba879aeeef73fccb"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:34:43 - BGCA:96]: Epoch 16 Avg epoch train loss: 2.27129 lr: [0.00016875, 0.00016875]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e519a0dbe46d42c3b696185d73edbbf4"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:35:02 - BGCA:96]: Epoch 17 Avg epoch train loss: 2.14246 lr: [0.0001575, 0.0001575]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d433b66482e496b9349485a6ce311be"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:35:20 - BGCA:96]: Epoch 18 Avg epoch train loss: 2.03404 lr: [0.00014625, 0.00014625]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eb4a220f3bd451cb2d2700258dee14b"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:35:38 - BGCA:96]: Epoch 19 Avg epoch train loss: 1.90548 lr: [0.000135, 0.000135]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf7aac2efbc46c69f762e5ac569f778"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:35:56 - BGCA:96]: Epoch 20 Avg epoch train loss: 1.83625 lr: [0.00012375, 0.00012375]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"680c601e74ce4e0c965e55e6ab12f7bd"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:36:14 - BGCA:96]: Epoch 21 Avg epoch train loss: 1.72325 lr: [0.00011250000000000001, 0.00011250000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"845270fb8d044df09d47996fc8bce7a4"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:36:32 - BGCA:96]: Epoch 22 Avg epoch train loss: 1.64517 lr: [0.00010125000000000001, 0.00010125000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e45b8e9557944b0b79b98fc39658978"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:36:50 - BGCA:96]: Epoch 23 Avg epoch train loss: 1.59055 lr: [9e-05, 9e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b6e7271e6054607ba3e2062b4a31992"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:37:09 - BGCA:96]: Epoch 24 Avg epoch train loss: 1.54530 lr: [7.875e-05, 7.875e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b51eda374e45fb9f42886378132294"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:37:27 - BGCA:96]: Epoch 25 Avg epoch train loss: 1.47636 lr: [6.75e-05, 6.75e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e83f3d0b55ad4924a52a15275bd3ac5e"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:37:45 - BGCA:96]: Epoch 26 Avg epoch train loss: 1.45000 lr: [5.6250000000000005e-05, 5.6250000000000005e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48b7d08fe5cd4c2ba63c9bcc3910d2db"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:38:03 - BGCA:96]: Epoch 27 Avg epoch train loss: 1.43690 lr: [4.5e-05, 4.5e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2494256deede4bdfadd7dbe2fbc29317"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:38:21 - BGCA:96]: Epoch 28 Avg epoch train loss: 1.37144 lr: [3.375e-05, 3.375e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69552b6568d942abbda76cfeec2c63b4"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:38:40 - BGCA:96]: Epoch 29 Avg epoch train loss: 1.35059 lr: [2.25e-05, 2.25e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b17972a7c5c34a178689fe0dc6c19d94"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:38:58 - BGCA:96]: Epoch 30 Avg epoch train loss: 1.35127 lr: [1.125e-05, 1.125e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/88 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a88dc034cb4dc4b25502c6be58ec9f"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:39:16 - BGCA:96]: Epoch 31 Avg epoch train loss: 1.30823 lr: [0.0, 0.0]\n[11/02/2023 16:39:17 - BGCA:104]: Save model checkpoint to aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/gene_aste-model\n[11/02/2023 16:39:17 - BGCA:106]: Finish training!\n[11/02/2023 16:39:18 - BGCA:87]: target-unlabel.txt\tTotal examples = 552 \n[11/02/2023 16:39:18 - BGCA:74]: Tag tokens: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n[11/02/2023 16:39:18 - BGCA:365]: 3793 target domain words\n[11/02/2023 16:39:18 - BGCA:135]: Inferencing on target_gene ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0ada399d1c4f29a0e259276551944f"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> 이 이 이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이이 <neg>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> 이순신 이순신 <opinion> <neg> 이순신은 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <neg> <neg>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos> 한국영화 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <neg>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <neg> 영화 <opinion> <neg> 영화 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos> 개그 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <neg> <neg>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <opinion> 100만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만만 <neg>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos> 은 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 아이 사랑 <opinion> 사랑의 사랑 <opinion> 사랑의 사랑 <opinion> 사랑의 사랑 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> 스토리 <opinion> <pos> 스토리 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> 카메라의의 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 장면 <pos> 카메라의 표현 <opinion> 연출 표현 <opinion> 화려의 표현 <opinion> 꽤나 <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <neg> <neg>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> 들 <opinion> <pos> 들 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> 연애 <opinion> <pos> 연애 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos> 그녀의 그녀의 그녀의 목소리를 <opinion>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <neg> <pos>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> <neg>\n[11/02/2023 16:42:47 - BGCA:32]: Output is invalid: <pos> 스토리 <opinion> <pos> 스토리 <opinion>\n[11/02/2023 16:42:47 - BGCA:33]: Filterd out 33 invalid samples\n[11/02/2023 16:42:51 - BGCA:381]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/extract_aste-model loaded.\n[11/02/2023 16:42:51 - BGCA:382]: Model emb weights of <pad> tensor([ 2.3119, -0.3872,  0.6557, -0.2127,  0.9099], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n[11/02/2023 16:42:52 - BGCA:74]: Tag tokens: ['<neg>', '<sep>', '<pos>', '<neu>', '<opinion>']\n[11/02/2023 16:42:52 - BGCA:135]: Inferencing on target_filter ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b5e60acee6e4d4199a8d1a1f722418a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:46:35 - BGCA:411]: 506 augmentations out of 519 are removed by model.\n[11/02/2023 16:46:35 - BGCA:195]: Aug num after filtering: 13\n[11/02/2023 16:46:35 - BGCA:206]: Aug num final: 13\n[11/02/2023 16:46:35 - BGCA:117]: ['train_aste', 'target_gene_aug'] is merged.\n[11/02/2023 16:46:35 - BGCA:118]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/train_aste+target_gene_aug_merged_processed.txt is written.\n[11/02/2023 16:46:35 - BGCA:159]: Use the same model.\n[11/02/2023 16:46:35 - BGCA:29]: ##############################################################\n[11/02/2023 16:46:35 - BGCA:30]: #################### Conduct aste Training####################\n[11/02/2023 16:46:35 - BGCA:31]: ##############################################################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87203b9a6c8b45a5b4dc518d75e4eae0"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:46:35 - BGCA:53]: Training examples out of 365:\n[11/02/2023 16:46:35 - BGCA:55]: Input : 상습 마약 투약 혐의를 받는 배우 유아인 씨가 재판에 넘겨졌습니다\n[11/02/2023 16:46:35 - BGCA:56]: Output: <neg> 배우 유아인 <opinion> 혐의를 받는\n[11/02/2023 16:46:35 - BGCA:55]: Input : 평소 구하기 힘든 주류 상품들을 합리적인 가격에 구매하고자 하는 고객들의 니즈가 지속되고 있기 때문이다\n[11/02/2023 16:46:35 - BGCA:56]: Output: <pos> 구하기 힘든 주류 상품 <opinion> 합리적인\n[11/02/2023 16:46:36 - BGCA:55]: Input : 상대평가를 전 과목으로 확대하는 내신 개편안은 학생들의 입시 부담을 폭증시킬 것이라고 우려했다\n[11/02/2023 16:46:36 - BGCA:56]: Output: <neg> 내신 개편안 <opinion> 입시 부담을 폭증시킬\n[11/02/2023 16:46:36 - BGCA:58]: Model emb weights of <pad> tensor([ 2.3219, -0.3943,  0.6443, -0.2159,  0.9259], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcbfba85f6e74138a26064652856337e"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:46:54 - BGCA:96]: Epoch 0 Avg epoch train loss: 1.20953 lr: [0.000348, 0.000348]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4b0216fa8aa46d2b19bfc995603d1ee"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:47:13 - BGCA:96]: Epoch 1 Avg epoch train loss: 0.46636 lr: [0.00033600000000000004, 0.00033600000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618916cf20b946598eb21f90eda59dd1"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:47:33 - BGCA:96]: Epoch 2 Avg epoch train loss: 0.36866 lr: [0.000324, 0.000324]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65db11c316f549689881210291300499"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:47:51 - BGCA:96]: Epoch 3 Avg epoch train loss: 0.34754 lr: [0.00031200000000000005, 0.00031200000000000005]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d1012650529460f9441ffc699862d9a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:48:10 - BGCA:96]: Epoch 4 Avg epoch train loss: 0.30265 lr: [0.00030000000000000003, 0.00030000000000000003]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333822e20d3849ea814103ac0eafe5d6"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:48:29 - BGCA:96]: Epoch 5 Avg epoch train loss: 0.24715 lr: [0.000288, 0.000288]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc9392475db4e8397834d8972867c49"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:48:48 - BGCA:96]: Epoch 6 Avg epoch train loss: 0.21217 lr: [0.00027600000000000004, 0.00027600000000000004]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a77ddf71be43bd967b02fcf765b5ce"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:49:06 - BGCA:96]: Epoch 7 Avg epoch train loss: 0.20553 lr: [0.000264, 0.000264]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc90096a1f2547659531e6378a9bd2c5"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:49:25 - BGCA:96]: Epoch 8 Avg epoch train loss: 0.15976 lr: [0.000252, 0.000252]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ccc56b59d546bfaddb90ef8bf485f6"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:49:44 - BGCA:96]: Epoch 9 Avg epoch train loss: 0.15381 lr: [0.00024, 0.00024]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b51cdfaf1434420a7d4e137255de694"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:50:03 - BGCA:96]: Epoch 10 Avg epoch train loss: 0.15034 lr: [0.000228, 0.000228]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f86f9781bef7400da1ecfc5d4a2d45e5"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:50:22 - BGCA:96]: Epoch 11 Avg epoch train loss: 0.12796 lr: [0.00021600000000000002, 0.00021600000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b02344111b741b9a4596b70f82b013a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:50:40 - BGCA:96]: Epoch 12 Avg epoch train loss: 0.11064 lr: [0.000204, 0.000204]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69f0786dac3f4319bbc52930f7354ba9"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:50:59 - BGCA:96]: Epoch 13 Avg epoch train loss: 0.09156 lr: [0.000192, 0.000192]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd7725dc6a464de49ae66818bb619465"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:51:18 - BGCA:96]: Epoch 14 Avg epoch train loss: 0.09428 lr: [0.00018, 0.00018]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33c43dad22d046e8b7d15bb1f50425da"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:51:37 - BGCA:96]: Epoch 15 Avg epoch train loss: 0.09866 lr: [0.00016800000000000002, 0.00016800000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1470472aa54d29829a70035c13dae3"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:51:56 - BGCA:96]: Epoch 16 Avg epoch train loss: 0.07036 lr: [0.00015600000000000002, 0.00015600000000000002]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5fa16807ace4f89a283784d55721620"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:52:15 - BGCA:96]: Epoch 17 Avg epoch train loss: 0.06113 lr: [0.000144, 0.000144]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3a291bf3d5e41bdb49d6d20a5ca2a5d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:52:33 - BGCA:96]: Epoch 18 Avg epoch train loss: 0.06468 lr: [0.000132, 0.000132]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ab3f3a28f5d4aadabf271c2d7375391"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:52:52 - BGCA:96]: Epoch 19 Avg epoch train loss: 0.06031 lr: [0.00012, 0.00012]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e51017e9466d4001be1a1316e9cf57ab"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:53:11 - BGCA:96]: Epoch 20 Avg epoch train loss: 0.06012 lr: [0.00010800000000000001, 0.00010800000000000001]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f39c5c69cde4b9b9be53ff2a365430b"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:53:30 - BGCA:96]: Epoch 21 Avg epoch train loss: 0.04901 lr: [9.6e-05, 9.6e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6165a6f40e4c4d31a604e7fc7530699d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:53:49 - BGCA:96]: Epoch 22 Avg epoch train loss: 0.05644 lr: [8.400000000000001e-05, 8.400000000000001e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3391357c12c48bf948c9cfe8bb201f5"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:54:07 - BGCA:96]: Epoch 23 Avg epoch train loss: 0.04005 lr: [7.2e-05, 7.2e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e729292b634c0a8b0134b628d2b96d"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:54:26 - BGCA:96]: Epoch 24 Avg epoch train loss: 0.04461 lr: [6e-05, 6e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a979ee6773847298fbabc0c6ddc2705"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:54:45 - BGCA:96]: Epoch 25 Avg epoch train loss: 0.04972 lr: [4.8e-05, 4.8e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd1ee64b48054ea6b4f42a979bf41733"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:55:04 - BGCA:96]: Epoch 26 Avg epoch train loss: 0.04739 lr: [3.6e-05, 3.6e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9da22d455d314fa1ad811b51e06ab69a"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:55:22 - BGCA:96]: Epoch 27 Avg epoch train loss: 0.03878 lr: [2.4e-05, 2.4e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8822924c3f27464aadfb2aa95a849c54"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:55:41 - BGCA:96]: Epoch 28 Avg epoch train loss: 0.04635 lr: [1.2e-05, 1.2e-05]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"138c3791891c4264a9183b9891b8ab2f"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:56:01 - BGCA:94]: Save model checkpoint to aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/checkpoint-e29\n[11/02/2023 16:56:01 - BGCA:96]: Epoch 29 Avg epoch train loss: 0.03195 lr: [0.0, 0.0]\n[11/02/2023 16:56:01 - BGCA:106]: Finish training!\n[11/02/2023 16:56:01 - BGCA:173]: ******************** Conduct Evaluating********************\n[11/02/2023 16:56:01 - BGCA:188]: We will perform validation on the following checkpoints: ['aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/checkpoint-e29']\n[11/02/2023 16:56:01 - BGCA:87]: test.txt\tTotal examples = 159 \n[11/02/2023 16:56:01 - BGCA:87]: dev.txt\tTotal examples = 50 \n[11/02/2023 16:56:01 - BGCA:87]: dev.txt\tTotal examples = 50 \n[11/02/2023 16:56:01 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/dev_aste_processed.txt is written.\n[11/02/2023 16:56:02 - BGCA:87]: test.txt\tTotal examples = 159 \n[11/02/2023 16:56:02 - BGCA:492]: aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/data/test_aste_processed.txt is written.\n[11/02/2023 16:56:02 - BGCA:203]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n[11/02/2023 16:56:02 - BGCA:204]: Decode by greedy\n[11/02/2023 16:56:02 - BGCA:216]: Load the trained model from aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/checkpoint-e29...\n[11/02/2023 16:56:05 - BGCA:27]: Eval set count dict: {'merge': 50}\n[11/02/2023 16:56:05 - BGCA:135]: Inferencing on dev_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"423ac88818c7416e955368b480e8ed93"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:56:19 - BGCA:360]: Gold: <neg> 박수홍의 형수 <opinion> 허위사실을 제공한\n[11/02/2023 16:56:19 - BGCA:361]: Gold list: [('박수홍의 형수', '허위사실을 제공한', 'negative')]\n[11/02/2023 16:56:19 - BGCA:362]: Pred: <neg> 이 사건은 <opinion> 혐의에서 지난 지난 지난 지난 지난 지난 지난 지난 지난 지난 지난 사건 <opinion> 혐의를 받는\n[11/02/2023 16:56:19 - BGCA:363]: Pred list: [('이 사건은', '혐의에서 지난 지난 지난 지난 지난 지난 지난 지난 지난 지난 지난 사건 <opinion> 혐의를 받는', 'negative')]\n[11/02/2023 16:56:19 - BGCA:360]: Gold: <neg> 수능 개편안 <opinion> 사교육 의존도를 심화시킬\n[11/02/2023 16:56:19 - BGCA:361]: Gold list: [('수능 개편안', '사교육 의존도를 심화시킬', 'negative')]\n[11/02/2023 16:56:19 - BGCA:362]: Pred: <neg> 수능 개편안 <opinion> 입시 부담을 심화시킬\n[11/02/2023 16:56:19 - BGCA:363]: Pred list: [('수능 개편안', '입시 부담을 심화시킬', 'negative')]\n[11/02/2023 16:56:19 - BGCA:360]: Gold: <pos> 땅굴 <opinion> 모두 갖추고 있다\n[11/02/2023 16:56:19 - BGCA:361]: Gold list: [('땅굴', '모두 갖추고 있다', 'positive')]\n[11/02/2023 16:56:19 - BGCA:362]: Pred: <pos> 팔레스타인 사람들이 <opinion> 다 아는\n[11/02/2023 16:56:19 - BGCA:363]: Pred list: [('팔레스타인 사람들이', '다 아는', 'positive')]\n[11/02/2023 16:56:19 - BGCA:364]: Results of raw output\n[11/02/2023 16:56:19 - BGCA:365]: {'precision': 0.016666666666666666, 'recall': 0.016129032258064516, 'f1': 0.01639344262295082}\n[11/02/2023 16:56:19 - BGCA:366]: Results of fixed output\n[11/02/2023 16:56:19 - BGCA:367]: {'precision': 0.0, 'recall': 0.0, 'f1': 0}\n[11/02/2023 16:56:19 - BGCA:27]: Eval set count dict: {'blog': 159}\n[11/02/2023 16:56:19 - BGCA:135]: Inferencing on test_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac1c05cf8cee46a6b1380092493801d9"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:57:10 - BGCA:243]: ====================================================================================================\n[11/02/2023 16:57:10 - BGCA:244]: dev result over from epochs ['29']\n[11/02/2023 16:57:10 - BGCA:246]: raw_scores\n[11/02/2023 16:57:10 - BGCA:248]: lang\tmerge\n[11/02/2023 16:57:10 - BGCA:258]: epoch-29[best]\t1.67/1.61/1.64/\t\n[11/02/2023 16:57:10 - BGCA:246]: fixed_scores\n[11/02/2023 16:57:10 - BGCA:248]: lang\tmerge\n[11/02/2023 16:57:10 - BGCA:258]: epoch-29[best]\t0.00/0.00/0.00/\t\n[11/02/2023 16:57:10 - BGCA:243]: ====================================================================================================\n[11/02/2023 16:57:10 - BGCA:244]: test result over from epochs ['29']\n[11/02/2023 16:57:10 - BGCA:246]: raw_scores\n[11/02/2023 16:57:10 - BGCA:248]: lang\tblog\n[11/02/2023 16:57:10 - BGCA:258]: epoch-29[best]\t1.24/1.06/1.15/\t\n[11/02/2023 16:57:10 - BGCA:246]: fixed_scores\n[11/02/2023 16:57:10 - BGCA:248]: lang\tblog\n[11/02/2023 16:57:10 - BGCA:258]: epoch-29[best]\t0.00/0.00/0.00/\t\n[11/02/2023 16:57:10 - BGCA:261]: The best checkpoint is aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/checkpoint-e29\n[11/02/2023 16:57:10 - BGCA:203]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n[11/02/2023 16:57:10 - BGCA:204]: Decode by constrained\n[11/02/2023 16:57:10 - BGCA:216]: Load the trained model from aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/checkpoint-e29...\n[11/02/2023 16:57:14 - BGCA:27]: Eval set count dict: {'merge': 50}\n[11/02/2023 16:57:14 - BGCA:135]: Inferencing on dev_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65ca4a6fa7f04ff4845089073f4283a1"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:57:32 - BGCA:360]: Gold: <neg> 박수홍의 형수 <opinion> 허위사실을 제공한\n[11/02/2023 16:57:32 - BGCA:361]: Gold list: [('박수홍의 형수', '허위사실을 제공한', 'negative')]\n[11/02/2023 16:57:32 - BGCA:362]: Pred: <neg> 박수홍의 형수 <opinion> 법적 대응을 예고한 바 있다\n[11/02/2023 16:57:32 - BGCA:363]: Pred list: [('박수홍의 형수', '법적 대응을 예고한 바 있다', 'negative')]\n[11/02/2023 16:57:32 - BGCA:360]: Gold: <neg> 수능 개편안 <opinion> 사교육 의존도를 심화시킬\n[11/02/2023 16:57:32 - BGCA:361]: Gold list: [('수능 개편안', '사교육 의존도를 심화시킬', 'negative')]\n[11/02/2023 16:57:32 - BGCA:362]: Pred: <neg> 수능 개편안 <opinion> 수능 대비 사교육을 심화시킬\n[11/02/2023 16:57:32 - BGCA:363]: Pred list: [('수능 개편안', '수능 대비 사교육을 심화시킬', 'negative')]\n[11/02/2023 16:57:32 - BGCA:360]: Gold: <pos> 땅굴 <opinion> 모두 갖추고 있다\n[11/02/2023 16:57:32 - BGCA:361]: Gold list: [('땅굴', '모두 갖추고 있다', 'positive')]\n[11/02/2023 16:57:32 - BGCA:362]: Pred: <pos> 하마스 땅굴 <opinion> 모두 갖추고 있다\n[11/02/2023 16:57:32 - BGCA:363]: Pred list: [('하마스 땅굴', '모두 갖추고 있다', 'positive')]\n[11/02/2023 16:57:32 - BGCA:364]: Results of raw output\n[11/02/2023 16:57:32 - BGCA:365]: {'precision': 0.04, 'recall': 0.03225806451612903, 'f1': 0.03571428571428571}\n[11/02/2023 16:57:32 - BGCA:366]: Results of fixed output\n[11/02/2023 16:57:32 - BGCA:367]: {'precision': 0.0, 'recall': 0.0, 'f1': 0}\n[11/02/2023 16:57:32 - BGCA:27]: Eval set count dict: {'blog': 159}\n[11/02/2023 16:57:32 - BGCA:135]: Inferencing on test_aste ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"590195ae09f54d7999c32d8b8ae9fafb"}},"metadata":{}},{"name":"stdout","text":"[11/02/2023 16:58:29 - BGCA:243]: ====================================================================================================\n[11/02/2023 16:58:29 - BGCA:244]: dev result over from epochs ['29']\n[11/02/2023 16:58:29 - BGCA:246]: raw_scores\n[11/02/2023 16:58:29 - BGCA:248]: lang\tmerge\n[11/02/2023 16:58:29 - BGCA:258]: epoch-29[best]\t4.00/3.23/3.57/\t\n[11/02/2023 16:58:29 - BGCA:246]: fixed_scores\n[11/02/2023 16:58:29 - BGCA:248]: lang\tmerge\n[11/02/2023 16:58:29 - BGCA:258]: epoch-29[best]\t0.00/0.00/0.00/\t\n[11/02/2023 16:58:29 - BGCA:243]: ====================================================================================================\n[11/02/2023 16:58:29 - BGCA:244]: test result over from epochs ['29']\n[11/02/2023 16:58:29 - BGCA:246]: raw_scores\n[11/02/2023 16:58:29 - BGCA:248]: lang\tblog\n[11/02/2023 16:58:29 - BGCA:258]: epoch-29[best]\t7.37/7.45/7.41/\t\n[11/02/2023 16:58:29 - BGCA:246]: fixed_scores\n[11/02/2023 16:58:29 - BGCA:248]: lang\tblog\n[11/02/2023 16:58:29 - BGCA:258]: epoch-29[best]\t0.53/0.53/0.53/\t\n[11/02/2023 16:58:29 - BGCA:261]: The best checkpoint is aste/cross_domain/1102_1522-experinments/seed-42/merge-blog/checkpoint-e29\n","output_type":"stream"}]},{"cell_type":"code","source":"collate_seed_results(absa_config, runed_dirs)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T16:58:29.878165Z","iopub.execute_input":"2023-11-02T16:58:29.878508Z","iopub.status.idle":"2023-11-02T16:58:29.913315Z","shell.execute_reply.started":"2023-11-02T16:58:29.878476Z","shell.execute_reply":"2023-11-02T16:58:29.912378Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[11/02/2023 16:58:29 - BGCA:277]: Averaging greedy\n[11/02/2023 16:58:29 - BGCA:416]: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n[11/02/2023 16:58:29 - BGCA:417]: Avged 1 runs raw_scores\n[11/02/2023 16:58:29 - BGCA:418]: blog-merge\tmerge-blog\tall\n[11/02/2023 16:58:29 - BGCA:420]: 0.88\t1.15\t1.01\n[11/02/2023 16:58:29 - BGCA:422]: 0.00\t0.00\t0.00\n[11/02/2023 16:58:29 - BGCA:416]: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n[11/02/2023 16:58:29 - BGCA:417]: Avged 1 runs fixed_scores\n[11/02/2023 16:58:29 - BGCA:418]: blog-merge\tmerge-blog\tall\n[11/02/2023 16:58:29 - BGCA:420]: 0.00\t0.00\t0.00\n[11/02/2023 16:58:29 - BGCA:422]: 0.00\t0.00\t0.00\n[11/02/2023 16:58:29 - BGCA:277]: Averaging constrained\n[11/02/2023 16:58:29 - BGCA:416]: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n[11/02/2023 16:58:29 - BGCA:417]: Avged 1 runs raw_scores\n[11/02/2023 16:58:29 - BGCA:418]: blog-merge\tmerge-blog\tall\n[11/02/2023 16:58:29 - BGCA:420]: 4.63\t7.41\t6.02\n[11/02/2023 16:58:29 - BGCA:422]: 0.00\t0.00\t0.00\n[11/02/2023 16:58:29 - BGCA:416]: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n[11/02/2023 16:58:29 - BGCA:417]: Avged 1 runs fixed_scores\n[11/02/2023 16:58:29 - BGCA:418]: blog-merge\tmerge-blog\tall\n[11/02/2023 16:58:29 - BGCA:420]: 0.00\t0.53\t0.26\n[11/02/2023 16:58:29 - BGCA:422]: 0.00\t0.00\t0.00\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}