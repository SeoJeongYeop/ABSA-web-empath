{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로더"
      ],
      "metadata": {
        "id": "c4zvCo9sCoR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 설치"
      ],
      "metadata": {
        "id": "m2HQg6f6YRCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "6wmwfdUzYQgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drive Mount"
      ],
      "metadata": {
        "id": "-0TqzAsJU5J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m1o4XP8LU4iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 자신의 경로로 설정\n",
        "BASE_DIR=\"/content/drive/MyDrive/Colab Notebooks/GW\""
      ],
      "metadata": {
        "id": "TgpTURAMVCCx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실행 테스트 코드"
      ],
      "metadata": {
        "id": "pnNHFxblCzlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라이브러리 Import"
      ],
      "metadata": {
        "id": "eiiSTSVgB68E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BartTokenizer\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from functools import cmp_to_key\n",
        "from itertools import chain"
      ],
      "metadata": {
        "id": "SWcUOB27CSBz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset, DataLoader 사용한 코드"
      ],
      "metadata": {
        "id": "aVXpHznRuKEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cmp_to_key 를 사용할 함수\n",
        "def cmp_aspect(v1, v2):\n",
        "    if v1[0]['from'] == v2[0]['from']:\n",
        "        return v1[1]['from'] - v2[1]['from']\n",
        "    return v1[0]['from'] - v2[0]['from']\n",
        "\n",
        "\n",
        "def cmp_opinion(v1, v2):\n",
        "    if v1[1]['from'] == v2[1]['from']:\n",
        "        return v1[0]['from'] - v2[0]['from']\n",
        "    return v1[1]['from'] - v2[1]['from']"
      ],
      "metadata": {
        "id": "HTpS4azueOQY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, opinion_first=True, limit=None):\n",
        "        super(ABSADataset, self).__init__()\n",
        "        self.limit = limit\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = self._load_data(path)\n",
        "        self.opinion_first = opinion_first\n",
        "\n",
        "        self.mapping = {\n",
        "            'POS': '<<positive>>',\n",
        "            'NEG': '<<negative>>',\n",
        "            'NEU': '<<neutral>>'\n",
        "        }\n",
        "        self.target_shift = len(self.mapping) + 2\n",
        "\n",
        "        cur_num_tokens = self.tokenizer.vocab_size\n",
        "        self.cur_num_token = cur_num_tokens\n",
        "\n",
        "        tokens_to_add = sorted(\n",
        "            list(self.mapping.values()),\n",
        "            key=lambda x: len(x),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        unique_no_split_tokens = self.tokenizer.unique_no_split_tokens\n",
        "        sorted_add_tokens = sorted(\n",
        "            list(tokens_to_add),\n",
        "            key=lambda x: len(x),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for tok in sorted_add_tokens:\n",
        "            assert self.tokenizer.convert_tokens_to_ids(\n",
        "                [tok])[0] == self.tokenizer.unk_token_id\n",
        "        self.tokenizer.unique_no_split_tokens = unique_no_split_tokens + sorted_add_tokens\n",
        "        self.tokenizer.add_tokens(sorted_add_tokens)\n",
        "        self.mapping2id = {}\n",
        "        self.mapping2targetid = {}\n",
        "\n",
        "        for key, value in self.mapping.items():\n",
        "            key_id = self.tokenizer.convert_tokens_to_ids(\n",
        "                self.tokenizer.tokenize(value))\n",
        "            assert len(key_id) == 1, value\n",
        "            assert key_id[0] >= cur_num_tokens\n",
        "            self.mapping2id[key] = key_id[0]\n",
        "            self.mapping2targetid[key] = len(self.mapping2targetid)\n",
        "\n",
        "    def _load_data(self, path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        data = data[:self.limit] if self.limit else data\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ins = self.data[idx]\n",
        "        raw_words = ins['words']\n",
        "        aspects = ins['aspects']\n",
        "        opinions = ins['opinions']\n",
        "        print(\"raw_words\", raw_words)\n",
        "        print(\"aspects\", aspects)\n",
        "        print(\"opinions\", opinions)\n",
        "\n",
        "        target, target_spans, src_tokens = self.prepare_target(ins)\n",
        "        return {\n",
        "            'src_tokens': src_tokens,\n",
        "            'tgt_tokens': target,\n",
        "            'target_span': target_spans,\n",
        "            'src_seq_len': len(src_tokens),\n",
        "            'tgt_seq_len': len(target)\n",
        "        }\n",
        "\n",
        "    def prepare_target(self, ins):\n",
        "        # Byte pair\n",
        "        raw_words = ins['raw_words']\n",
        "        word_bpes = [[self.tokenizer.bos_token_id]]\n",
        "        for word in raw_words:\n",
        "            bpes = self.tokenizer.tokenize(word, add_prefix_space=True)\n",
        "            bpes = self.tokenizer.convert_tokens_to_ids(bpes)\n",
        "            word_bpes.append(bpes)\n",
        "        word_bpes.append([self.tokenizer.eos_token_id])\n",
        "\n",
        "        lens = list(map(len, word_bpes))\n",
        "        cum_lens = np.cumsum(list(lens)).tolist()\n",
        "        target = [0]  # sos를 위해 0 추가\n",
        "        target_spans = []\n",
        "\n",
        "        aspects_opinions = [(a, o)\n",
        "                            for a, o in zip(ins['aspects'], ins['opinions'])]\n",
        "        if self.opinion_first:\n",
        "            aspects_opinions = sorted(\n",
        "                aspects_opinions, key=cmp_to_key(cmp_opinion))\n",
        "        else:\n",
        "            aspects_opinions = sorted(\n",
        "                aspects_opinions, key=cmp_to_key(cmp_aspect))\n",
        "\n",
        "        for aspects, opinions in aspects_opinions:\n",
        "            # bpe의 start를 예측\n",
        "            print(\"aspects\", aspects)\n",
        "            print(\"opinions\", opinions)\n",
        "            assert aspects['index'] == opinions['index']\n",
        "\n",
        "            a_start_bpe = cum_lens[aspects['from']]\n",
        "            a_end_bpe = cum_lens[aspects['to']-1]\n",
        "\n",
        "            o_start_bpe = cum_lens[opinions['from']]\n",
        "            o_end_bpe = cum_lens[opinions['to']-1]\n",
        "\n",
        "            if self.opinion_first:\n",
        "                target_spans.append([o_start_bpe+self.target_shift, o_end_bpe+self.target_shift,\n",
        "                                     a_start_bpe+self.target_shift, a_end_bpe+self.target_shift])\n",
        "            else:\n",
        "                target_spans.append([a_start_bpe+self.target_shift, a_end_bpe+self.target_shift,\n",
        "                                     o_start_bpe+self.target_shift, o_end_bpe+self.target_shift])\n",
        "            print(\"target_spans\", target_spans)\n",
        "\n",
        "            # 앞에 sos랑 eos 포함\n",
        "            target_spans[-1].append(self.mapping2targetid[aspects['polarity']]+2)\n",
        "            target_spans[-1] = tuple(target_spans[-1])\n",
        "        target.extend(list(chain(*target_spans)))\n",
        "        target.append(1)  # eos를 위해 1을 추가\n",
        "        print(\"target\", target)\n",
        "\n",
        "        return {'tgt_tokens': target, 'target_span': target_spans, 'src_tokens': list(chain(*word_bpes))}"
      ],
      "metadata": {
        "id": "WzoPHaKN50zY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 설정\n",
        "data_path = '/data/train_convert.json'\n",
        "print(BASE_DIR + data_path)\n",
        "# Pretrained Model 설정\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "6KeUjhekH2yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', add_prefix_space=True)"
      ],
      "metadata": {
        "id": "1L1-1Ed-JgrE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ABSADataset(BASE_DIR + data_path, tokenizer, limit=10)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Rg9g26934hD3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in dataloader:\n",
        "    print(\"Batch\", batch)\n",
        "\n",
        "print(\"DONE\")"
      ],
      "metadata": {
        "id": "JKsxQsjWVZSj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}